{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79678e7-897c-492c-b8c2-de0efc2738ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/iharsawi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import hdbscan\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654cb08f-694a-4b5d-9e5b-c26d1cf91a81",
   "metadata": {},
   "source": [
    "## Load Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efc4f23-d838-423e-aed1-59c0722533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5714, 21)\n",
      "(5726, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON files\n",
    "with open('transcripts_BEFORE_COVID.json', 'r') as file:\n",
    "    before_covid_data = json.load(file)\n",
    "\n",
    "with open('transcripts_AFTER_COVID.json', 'r') as file:\n",
    "    after_covid_data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "before_covid_df = pd.json_normalize(before_covid_data)\n",
    "after_covid_df = pd.json_normalize(after_covid_data)\n",
    "\n",
    "print(before_covid_df.shape)\n",
    "print(after_covid_df.shape)\n",
    "\n",
    "# Extract necessary columns\n",
    "texts_before = before_covid_df[['transcripts', 'channel.ideology']]\n",
    "texts_after = after_covid_df[['transcripts', 'channel.ideology']]\n",
    "\n",
    "# Get unique ideologies\n",
    "ideologies = texts_before['channel.ideology'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b5d67-72cb-4d2d-b7cd-ad98edea308c",
   "metadata": {},
   "source": [
    "## Load Summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5d65b8-9e79-43cd-a539-622b7c6e29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5714, 22)\n",
      "(5726, 22)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON files\n",
    "with open('summarized_BEFORE_COVID.json', 'r') as file:\n",
    "    before_covid_data_summarized = json.load(file)\n",
    "\n",
    "with open('summarized_AFTER_COVID.json', 'r') as file:\n",
    "    after_covid_data_summarized = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "before_covid_df_summarized = pd.json_normalize(before_covid_data_summarized)\n",
    "after_covid_df_summarized = pd.json_normalize(after_covid_data_summarized)\n",
    "\n",
    "print(before_covid_df_summarized.shape)\n",
    "print(after_covid_df_summarized.shape)\n",
    "\n",
    "# Extract necessary columns\n",
    "texts_before_summarized = before_covid_df_summarized[['summary', 'channel.ideology']]\n",
    "texts_after_summarized = after_covid_df_summarized[['summary', 'channel.ideology']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa504f-1593-4bb3-9290-6ffe78dc8d81",
   "metadata": {},
   "source": [
    "## Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e65ea14-f679-4724-afc9-e097a3559ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2065 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token size for texts_before: 1819.2051102555129\n",
      "Average token size for texts_after: 1841.8499825358017\n",
      "Average token size for texts_before_summarized: 247.6641582079104\n",
      "Average token size for texts_after_summarized: 246.74886482710443\n",
      "Max token size for texts_before: 5605\n",
      "Max token size for texts_after: 6016\n",
      "Max token size for texts_before_summarized: 452\n",
      "Max token size for texts_after_summarized: 461\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to calculate average token size\n",
    "def get_average_token_size(texts, tokenizer):\n",
    "    total_tokens = 0\n",
    "    total_texts = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    average_size = total_tokens / total_texts if total_texts > 0 else 0\n",
    "    return average_size\n",
    "\n",
    "# Calculate average token size for texts_before and texts_after\n",
    "average_token_size_before = get_average_token_size(texts_before['transcripts'], tokenizer)\n",
    "average_token_size_after = get_average_token_size(texts_after['transcripts'], tokenizer)\n",
    "average_token_size_before_summarized = get_average_token_size(texts_before_summarized['summary'], tokenizer)\n",
    "average_token_size_after_summarized = get_average_token_size(texts_after_summarized['summary'], tokenizer)\n",
    "\n",
    "print(f'Average token size for texts_before: {average_token_size_before}')\n",
    "print(f'Average token size for texts_after: {average_token_size_after}')\n",
    "print(f'Average token size for texts_before_summarized: {average_token_size_before_summarized}')\n",
    "print(f'Average token size for texts_after_summarized: {average_token_size_after_summarized}')\n",
    "\n",
    "# Function to calculate max token size\n",
    "def get_max_token_size(texts, tokenizer):\n",
    "    max_size = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        max_size = max(max_size, len(tokens))\n",
    "    return max_size\n",
    "\n",
    "# Calculate max token size for texts_before and texts_after\n",
    "max_token_size_before = get_max_token_size(texts_before['transcripts'], tokenizer)\n",
    "max_token_size_after = get_max_token_size(texts_after['transcripts'], tokenizer)\n",
    "max_token_size_before_summarized = get_max_token_size(texts_before_summarized['summary'], tokenizer)\n",
    "max_token_size_after_summarized = get_max_token_size(texts_after_summarized['summary'], tokenizer)\n",
    "\n",
    "print(f'Max token size for texts_before: {max_token_size_before}')\n",
    "print(f'Max token size for texts_after: {max_token_size_after}')\n",
    "print(f'Max token size for texts_before_summarized: {max_token_size_before_summarized}')\n",
    "print(f'Max token size for texts_after_summarized: {max_token_size_after_summarized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e00afc-b1e8-4a36-963b-e9eb8f87316a",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3728b13b-32a8-4501-ba88-123e4d603878",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_before = texts_before[texts_before['channel.ideology'] == \"BLACK\"]\n",
    "texts_after = texts_after[texts_after['channel.ideology'] == \"BLACK\"]\n",
    "texts_before_summarized = texts_before_summarized[texts_before_summarized['channel.ideology'] == \"BLACK\"]\n",
    "texts_after_summarized = texts_after_summarized[texts_after_summarized['channel.ideology'] == \"BLACK\"]\n",
    "\n",
    "# List of ideologies\n",
    "ideologies = [\"BLACK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe485c-5744-489b-acb6-4ab3b4334d5b",
   "metadata": {},
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ffb24b-36af-43bb-824e-8ed5c4bb26c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n",
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>183</td>\n",
       "      <td>-1_the_and_to_you</td>\n",
       "      <td>[the, and, to, you, of, that, in, is, it, this]</td>\n",
       "      <td>[hey everybody how you doing?  i'm dr boyce wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0_to_and_the_that</td>\n",
       "      <td>[to, and, the, that, you, she, her, this, of, ...</td>\n",
       "      <td>[I cannot believe.  But the tenant already liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1_you_so_the_to</td>\n",
       "      <td>[you, so, the, to, and, is, in, of, like, its]</td>\n",
       "      <td>[Hey guys, welcome back to my channel.  My nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2_the_and_of_to</td>\n",
       "      <td>[the, and, of, to, that, is, you, in, this, they]</td>\n",
       "      <td>[unleash the nihilists and the atheists, and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>3_and_to_the_black</td>\n",
       "      <td>[and, to, the, black, that, in, they, of, you,...</td>\n",
       "      <td>[Here we are back on the Connecting the Dots Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4_the_you_to_of</td>\n",
       "      <td>[the, you, to, of, what, and, that, is, right,...</td>\n",
       "      <td>[Come over here sister.  Come talk to me siste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>5_to_the_and_you</td>\n",
       "      <td>[to, the, and, you, that, on, of, im, out, in]</td>\n",
       "      <td>[What's going on, guys?  Get these white glass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>6_you_and_to_know</td>\n",
       "      <td>[you, and, to, know, that, the, it, chick, get...</td>\n",
       "      <td>[what's happening.  fam lar movement still mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7_and_the_to_in</td>\n",
       "      <td>[and, the, to, in, was, you, that, this, lord,...</td>\n",
       "      <td>[Hi guys, this is your sister Karen Gidden in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8_to_the_that_you</td>\n",
       "      <td>[to, the, that, you, of, and, is, it, they, have]</td>\n",
       "      <td>[B-1 Brigadiers, it should come as no surprise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                Name  \\\n",
       "0     -1    183   -1_the_and_to_you   \n",
       "1      0     60   0_to_and_the_that   \n",
       "2      1     52     1_you_so_the_to   \n",
       "3      2     41     2_the_and_of_to   \n",
       "4      3     37  3_and_to_the_black   \n",
       "5      4     29     4_the_you_to_of   \n",
       "6      5     19    5_to_the_and_you   \n",
       "7      6     13   6_you_and_to_know   \n",
       "8      7     10     7_and_the_to_in   \n",
       "9      8     10   8_to_the_that_you   \n",
       "\n",
       "                                      Representation  \\\n",
       "0    [the, and, to, you, of, that, in, is, it, this]   \n",
       "1  [to, and, the, that, you, she, her, this, of, ...   \n",
       "2     [you, so, the, to, and, is, in, of, like, its]   \n",
       "3  [the, and, of, to, that, is, you, in, this, they]   \n",
       "4  [and, to, the, black, that, in, they, of, you,...   \n",
       "5  [the, you, to, of, what, and, that, is, right,...   \n",
       "6     [to, the, and, you, that, on, of, im, out, in]   \n",
       "7  [you, and, to, know, that, the, it, chick, get...   \n",
       "8  [and, the, to, in, was, you, that, this, lord,...   \n",
       "9  [to, the, that, you, of, and, is, it, they, have]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [hey everybody how you doing?  i'm dr boyce wa...  \n",
       "1  [I cannot believe.  But the tenant already liv...  \n",
       "2  [Hey guys, welcome back to my channel.  My nam...  \n",
       "3  [unleash the nihilists and the atheists, and w...  \n",
       "4  [Here we are back on the Connecting the Dots Y...  \n",
       "5  [Come over here sister.  Come talk to me siste...  \n",
       "6  [What's going on, guys?  Get these white glass...  \n",
       "7  [what's happening.  fam lar movement still mov...  \n",
       "8  [Hi guys, this is your sister Karen Gidden in ...  \n",
       "9  [B-1 Brigadiers, it should come as no surprise...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>-1_the_and_to_you</td>\n",
       "      <td>[the, and, to, you, that, of, is, in, this, it]</td>\n",
       "      <td>[In the upcoming presentation, we will be disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>0_to_the_and_you</td>\n",
       "      <td>[to, the, and, you, that, of, in, this, is, it]</td>\n",
       "      <td>[If you are not familiar with the concepts of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>1_the_and_to_you</td>\n",
       "      <td>[the, and, to, you, of, that, is, in, it, this]</td>\n",
       "      <td>[Good rising brethren, this is Big Judah comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>2_the_to_and_of</td>\n",
       "      <td>[the, to, and, of, you, in, that, we, is, are]</td>\n",
       "      <td>[hey guys welcome back to my channel.  thank y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count               Name  \\\n",
       "0     -1     42  -1_the_and_to_you   \n",
       "1      0    228   0_to_the_and_you   \n",
       "2      1    136   1_the_and_to_you   \n",
       "3      2     58    2_the_to_and_of   \n",
       "\n",
       "                                    Representation  \\\n",
       "0  [the, and, to, you, that, of, is, in, this, it]   \n",
       "1  [to, the, and, you, that, of, in, this, is, it]   \n",
       "2  [the, and, to, you, of, that, is, in, it, this]   \n",
       "3   [the, to, and, of, you, in, that, we, is, are]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [In the upcoming presentation, we will be disc...  \n",
       "1  [If you are not familiar with the concepts of ...  \n",
       "2  [Good rising brethren, this is Big Judah comin...  \n",
       "3  [hey guys welcome back to my channel.  thank y...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ada14d-021c-408e-9dab-ad7c24417ce1",
   "metadata": {},
   "source": [
    "## Consider basic \"pre-processing tricks + cTF-IDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6b90ad-bd8e-4d25-84c7-30a57cf12e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n",
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n",
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>151</td>\n",
       "      <td>-1_know_dont_going_im</td>\n",
       "      <td>[know, dont, going, im, like, just, people, th...</td>\n",
       "      <td>[It was really bad.  That was the whole focus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0_police_know_dont_just</td>\n",
       "      <td>[police, know, dont, just, like, im, going, sh...</td>\n",
       "      <td>[Hey, family.  Happy Friday.  This is starting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1_africa_oh_yeah_oh oh</td>\n",
       "      <td>[africa, oh, yeah, oh oh, thank, oh oh oh, lik...</td>\n",
       "      <td>[Please subscribe.  Comment down below.  Smash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2_high_people_going_right</td>\n",
       "      <td>[high, people, going, right, bethel, know, jus...</td>\n",
       "      <td>[Hey, this is JT.  Another episode about life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>3_black_white_folk_people</td>\n",
       "      <td>[black, white, folk, people, black folk, black...</td>\n",
       "      <td>[Hello.  Welcome again to Connecting the Dots....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4_right_bible_god_read</td>\n",
       "      <td>[right, bible, god, read, verse, man, thats, p...</td>\n",
       "      <td>[Look, there's the Black Holocaust.  I knew it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>5_yeah_yall_whats_im</td>\n",
       "      <td>[yeah, yall, whats, im, shit, going, got, man,...</td>\n",
       "      <td>[yo hello all right we're here.  what's going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>6_chick_lame_know_lame chick</td>\n",
       "      <td>[chick, lame, know, lame chick, like, dont, ju...</td>\n",
       "      <td>[You know, the Me Too movement have turned a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7_vision_lord_beyonce_brothers sisters</td>\n",
       "      <td>[vision, lord, beyonce, brothers sisters, sist...</td>\n",
       "      <td>[Just with this quick update, Hi guys, this is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>8_bank america_bank_youtube_respect</td>\n",
       "      <td>[bank america, bank, youtube, respect, white, ...</td>\n",
       "      <td>[Forgive me, brothers.  It has been over a wee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                    Name  \\\n",
       "0     -1    151                   -1_know_dont_going_im   \n",
       "1      0     62                 0_police_know_dont_just   \n",
       "2      1     57                  1_africa_oh_yeah_oh oh   \n",
       "3      2     54               2_high_people_going_right   \n",
       "4      3     43               3_black_white_folk_people   \n",
       "5      4     28                  4_right_bible_god_read   \n",
       "6      5     23                    5_yeah_yall_whats_im   \n",
       "7      6     12            6_chick_lame_know_lame chick   \n",
       "8      7     12  7_vision_lord_beyonce_brothers sisters   \n",
       "9      8     12     8_bank america_bank_youtube_respect   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, dont, going, im, like, just, people, th...   \n",
       "1  [police, know, dont, just, like, im, going, sh...   \n",
       "2  [africa, oh, yeah, oh oh, thank, oh oh oh, lik...   \n",
       "3  [high, people, going, right, bethel, know, jus...   \n",
       "4  [black, white, folk, people, black folk, black...   \n",
       "5  [right, bible, god, read, verse, man, thats, p...   \n",
       "6  [yeah, yall, whats, im, shit, going, got, man,...   \n",
       "7  [chick, lame, know, lame chick, like, dont, ju...   \n",
       "8  [vision, lord, beyonce, brothers sisters, sist...   \n",
       "9  [bank america, bank, youtube, respect, white, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [It was really bad.  That was the whole focus ...  \n",
       "1  [Hey, family.  Happy Friday.  This is starting...  \n",
       "2  [Please subscribe.  Comment down below.  Smash...  \n",
       "3  [Hey, this is JT.  Another episode about life ...  \n",
       "4  [Hello.  Welcome again to Connecting the Dots....  \n",
       "5  [Look, there's the Black Holocaust.  I knew it...  \n",
       "6  [yo hello all right we're here.  what's going ...  \n",
       "7  [You know, the Me Too movement have turned a l...  \n",
       "8  [Just with this quick update, Hi guys, this is...  \n",
       "9  [Forgive me, brothers.  It has been over a wee...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>54</td>\n",
       "      <td>-1_jehovahs_know_going_witnesses</td>\n",
       "      <td>[jehovahs, know, going, witnesses, im, royal, ...</td>\n",
       "      <td>[How's the royal family?  I pray that everyone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>0_black_like_im_dont</td>\n",
       "      <td>[black, like, im, dont, know, just, going, yal...</td>\n",
       "      <td>[hey yo what up youtube y'all know who it is. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>1_right_lord_know_going</td>\n",
       "      <td>[right, lord, know, going, people, thats, like...</td>\n",
       "      <td>[Good rising, brethren.  This is Big Judah, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>2_africa_african_chinese_people</td>\n",
       "      <td>[africa, african, chinese, people, thank, know...</td>\n",
       "      <td>[Otherwise you are going to ask and then answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3_know_son_kids_like</td>\n",
       "      <td>[know, son, kids, like, family, dad, want, jus...</td>\n",
       "      <td>[What's happening, fam?  LA y'all movement sti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                              Name  \\\n",
       "0     -1     54  -1_jehovahs_know_going_witnesses   \n",
       "1      0    205              0_black_like_im_dont   \n",
       "2      1    136           1_right_lord_know_going   \n",
       "3      2     57   2_africa_african_chinese_people   \n",
       "4      3     12              3_know_son_kids_like   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [jehovahs, know, going, witnesses, im, royal, ...   \n",
       "1  [black, like, im, dont, know, just, going, yal...   \n",
       "2  [right, lord, know, going, people, thats, like...   \n",
       "3  [africa, african, chinese, people, thank, know...   \n",
       "4  [know, son, kids, like, family, dad, want, jus...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [How's the royal family?  I pray that everyone...  \n",
       "1  [hey yo what up youtube y'all know who it is. ...  \n",
       "2  [Good rising, brethren.  This is Big Judah, co...  \n",
       "3  [Otherwise you are going to ask and then answe...  \n",
       "4  [What's happening, fam?  LA y'all movement sti...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698a9d6-fce9-4709-bf98-13695bce0f21",
   "metadata": {},
   "source": [
    "## Consider \"fine-tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded9ce6f-3313-465c-af69-570956870fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n",
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n",
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>150</td>\n",
       "      <td>-1_just_know_like_dont</td>\n",
       "      <td>[just, know, like, dont, going, im, people, th...</td>\n",
       "      <td>[bethel, basically, place, family, yall, bad, ...</td>\n",
       "      <td>[like, dont, im, thats, black, time, look, say...</td>\n",
       "      <td>[This is your brother Malcolm coming at you wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0_police_know_dont_im</td>\n",
       "      <td>[police, know, dont, im, just, like, going, th...</td>\n",
       "      <td>[ashley, jay, situation, shes, investigation, ...</td>\n",
       "      <td>[police, like, black, shes, think, family, did...</td>\n",
       "      <td>[How did you know they were fleeing?  Good det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1_high_people_going_shall</td>\n",
       "      <td>[high, people, going, shall, right, book, chap...</td>\n",
       "      <td>[scripture, scriptures, chapter verse, verse, ...</td>\n",
       "      <td>[high, verse, bible, read, messiah, god, truth...</td>\n",
       "      <td>[Thy kingdom come, thy will be done, on earth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>2_africa_oh_yeah_oh oh</td>\n",
       "      <td>[africa, oh, yeah, oh oh, oh oh oh, thank, lik...</td>\n",
       "      <td>[blacksit family, blacksit, come africa, afric...</td>\n",
       "      <td>[africa, oh oh oh, ghana, african, say, nigeri...</td>\n",
       "      <td>[Please subscribe.  Comment down below.  Smash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>3_black_white_folk_black folk</td>\n",
       "      <td>[black, white, folk, black folk, black people,...</td>\n",
       "      <td>[blacks, black folk, white supremacy, black pe...</td>\n",
       "      <td>[black folk, black people, vote, race, constit...</td>\n",
       "      <td>[Hello.  Welcome again to Connecting the Dots....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4_god_right_bible_read</td>\n",
       "      <td>[god, right, bible, read, verse, man, thats, p...</td>\n",
       "      <td>[jeremiah, tribe, according bible, bible says,...</td>\n",
       "      <td>[god, bible, verse, israel, chapter, white, sa...</td>\n",
       "      <td>[Look, there's the Black Holocaust.  I knew it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>5_shut shut_shut shut shut_shut_yeah</td>\n",
       "      <td>[shut shut, shut shut shut, shut, yeah, yall, ...</td>\n",
       "      <td>[whats going, door, talking, yall, yeah im, ge...</td>\n",
       "      <td>[shut shut shut, yall, im, shit, like, monday ...</td>\n",
       "      <td>[yo hello all right we're here.  what's going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>6_chick_know_women_lame</td>\n",
       "      <td>[chick, know, women, lame, like, men, lame chi...</td>\n",
       "      <td>[just lame chick, lame chick, women, everybody...</td>\n",
       "      <td>[chick, women, lame, like, lame chick, theyre,...</td>\n",
       "      <td>[That's why the pimps say hoes come going beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7_vision_lord_brothers sisters_sisters</td>\n",
       "      <td>[vision, lord, brothers sisters, sisters, beyo...</td>\n",
       "      <td>[sister karen, vision lord, pastor, amen bless...</td>\n",
       "      <td>[lord, brothers sisters, benny, benny hinn, ch...</td>\n",
       "      <td>[Hello, this is Risa Sukhari again in Jesus Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8_bank america_bank_youtube_respect</td>\n",
       "      <td>[bank america, bank, youtube, respect, white, ...</td>\n",
       "      <td>[white supremacists, white supremacist, white ...</td>\n",
       "      <td>[bank america, bank, youtube, respect, black, ...</td>\n",
       "      <td>[B-1 Brigadiers, it should come as no surprise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                    Name  \\\n",
       "0     -1    150                  -1_just_know_like_dont   \n",
       "1      0     62                   0_police_know_dont_im   \n",
       "2      1     61               1_high_people_going_shall   \n",
       "3      2     52                  2_africa_oh_yeah_oh oh   \n",
       "4      3     37           3_black_white_folk_black folk   \n",
       "5      4     28                  4_god_right_bible_read   \n",
       "6      5     25    5_shut shut_shut shut shut_shut_yeah   \n",
       "7      6     16                 6_chick_know_women_lame   \n",
       "8      7     13  7_vision_lord_brothers sisters_sisters   \n",
       "9      8     10     8_bank america_bank_youtube_respect   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [just, know, like, dont, going, im, people, th...   \n",
       "1  [police, know, dont, im, just, like, going, th...   \n",
       "2  [high, people, going, shall, right, book, chap...   \n",
       "3  [africa, oh, yeah, oh oh, oh oh oh, thank, lik...   \n",
       "4  [black, white, folk, black folk, black people,...   \n",
       "5  [god, right, bible, read, verse, man, thats, p...   \n",
       "6  [shut shut, shut shut shut, shut, yeah, yall, ...   \n",
       "7  [chick, know, women, lame, like, men, lame chi...   \n",
       "8  [vision, lord, brothers sisters, sisters, beyo...   \n",
       "9  [bank america, bank, youtube, respect, white, ...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [bethel, basically, place, family, yall, bad, ...   \n",
       "1  [ashley, jay, situation, shes, investigation, ...   \n",
       "2  [scripture, scriptures, chapter verse, verse, ...   \n",
       "3  [blacksit family, blacksit, come africa, afric...   \n",
       "4  [blacks, black folk, white supremacy, black pe...   \n",
       "5  [jeremiah, tribe, according bible, bible says,...   \n",
       "6  [whats going, door, talking, yall, yeah im, ge...   \n",
       "7  [just lame chick, lame chick, women, everybody...   \n",
       "8  [sister karen, vision lord, pastor, amen bless...   \n",
       "9  [white supremacists, white supremacist, white ...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [like, dont, im, thats, black, time, look, say...   \n",
       "1  [police, like, black, shes, think, family, did...   \n",
       "2  [high, verse, bible, read, messiah, god, truth...   \n",
       "3  [africa, oh oh oh, ghana, african, say, nigeri...   \n",
       "4  [black folk, black people, vote, race, constit...   \n",
       "5  [god, bible, verse, israel, chapter, white, sa...   \n",
       "6  [shut shut shut, yall, im, shit, like, monday ...   \n",
       "7  [chick, women, lame, like, lame chick, theyre,...   \n",
       "8  [lord, brothers sisters, benny, benny hinn, ch...   \n",
       "9  [bank america, bank, youtube, respect, black, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [This is your brother Malcolm coming at you wi...  \n",
       "1  [How did you know they were fleeing?  Good det...  \n",
       "2  [Thy kingdom come, thy will be done, on earth ...  \n",
       "3  [Please subscribe.  Comment down below.  Smash...  \n",
       "4  [Hello.  Welcome again to Connecting the Dots....  \n",
       "5  [Look, there's the Black Holocaust.  I knew it...  \n",
       "6  [yo hello all right we're here.  what's going ...  \n",
       "7  [That's why the pimps say hoes come going beca...  \n",
       "8  [Hello, this is Risa Sukhari again in Jesus Ch...  \n",
       "9  [B-1 Brigadiers, it should come as no surprise...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>96</td>\n",
       "      <td>-1_know_just_im_people</td>\n",
       "      <td>[know, just, im, people, like, dont, going, je...</td>\n",
       "      <td>[jehovahs witness, jehovahs witnesses, jehovah...</td>\n",
       "      <td>[im, dont, jehovahs, family, witnesses, theyre...</td>\n",
       "      <td>[Some in the community may feel uneasy as we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>0_right_lord_going_know</td>\n",
       "      <td>[right, lord, going, know, people, thats, oh, ...</td>\n",
       "      <td>[verse, pray, israelites, judah, shalom, bible...</td>\n",
       "      <td>[israel, high, lord oh lord, god, shall, verse...</td>\n",
       "      <td>[McCurran boys!  Father in the sky, I shed tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>1_im_like_know_yall</td>\n",
       "      <td>[im, like, know, yall, yeah, got, police, shit...</td>\n",
       "      <td>[officer, police, cop, officers, cops, attorne...</td>\n",
       "      <td>[like, yall, police, shit, car, niggas, home, ...</td>\n",
       "      <td>[I told y'all that when I get hot and I get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>2_black_white_women_men</td>\n",
       "      <td>[black, white, women, men, people, dont, man, ...</td>\n",
       "      <td>[black community, white supremacy, black men, ...</td>\n",
       "      <td>[black, man, black men, mean, black women, wan...</td>\n",
       "      <td>[hey yo what up youtube y'all know who it is. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>3_africa_african_chinese_thank</td>\n",
       "      <td>[africa, african, chinese, thank, people, know...</td>\n",
       "      <td>[african countries, africans, africa, african,...</td>\n",
       "      <td>[africa, african, chinese, continent, countrie...</td>\n",
       "      <td>[Hello.  Welcome to Connecting the Dots on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4_brothers sisters_sisters_vision_brothers</td>\n",
       "      <td>[brothers sisters, sisters, vision, brothers, ...</td>\n",
       "      <td>[visions, darkness, sister karen, prayer, days...</td>\n",
       "      <td>[brothers sisters, spirit, clay, said, amen, k...</td>\n",
       "      <td>[Hi guys, this is your sister Karen Gidden in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>5_know_like_kids_son</td>\n",
       "      <td>[know, like, kids, son, dont, family, dad, wan...</td>\n",
       "      <td>[new age family, age family, mothers, family, ...</td>\n",
       "      <td>[like, kids, family, say, father, mother, age,...</td>\n",
       "      <td>[What's happening, fam?  LA y'all movement sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>6_women_want_youre_woman</td>\n",
       "      <td>[women, want, youre, woman, shes, traditional,...</td>\n",
       "      <td>[inner beauty movement, want women, modern wom...</td>\n",
       "      <td>[women, shes, traditional, married, feminine, ...</td>\n",
       "      <td>[Hello, I'm Nicole Michelle, founder and femin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                        Name  \\\n",
       "0     -1     96                      -1_know_just_im_people   \n",
       "1      0    105                     0_right_lord_going_know   \n",
       "2      1     78                         1_im_like_know_yall   \n",
       "3      2     74                     2_black_white_women_men   \n",
       "4      3     61              3_africa_african_chinese_thank   \n",
       "5      4     22  4_brothers sisters_sisters_vision_brothers   \n",
       "6      5     14                        5_know_like_kids_son   \n",
       "7      6     14                    6_women_want_youre_woman   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, just, im, people, like, dont, going, je...   \n",
       "1  [right, lord, going, know, people, thats, oh, ...   \n",
       "2  [im, like, know, yall, yeah, got, police, shit...   \n",
       "3  [black, white, women, men, people, dont, man, ...   \n",
       "4  [africa, african, chinese, thank, people, know...   \n",
       "5  [brothers sisters, sisters, vision, brothers, ...   \n",
       "6  [know, like, kids, son, dont, family, dad, wan...   \n",
       "7  [women, want, youre, woman, shes, traditional,...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [jehovahs witness, jehovahs witnesses, jehovah...   \n",
       "1  [verse, pray, israelites, judah, shalom, bible...   \n",
       "2  [officer, police, cop, officers, cops, attorne...   \n",
       "3  [black community, white supremacy, black men, ...   \n",
       "4  [african countries, africans, africa, african,...   \n",
       "5  [visions, darkness, sister karen, prayer, days...   \n",
       "6  [new age family, age family, mothers, family, ...   \n",
       "7  [inner beauty movement, want women, modern wom...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [im, dont, jehovahs, family, witnesses, theyre...   \n",
       "1  [israel, high, lord oh lord, god, shall, verse...   \n",
       "2  [like, yall, police, shit, car, niggas, home, ...   \n",
       "3  [black, man, black men, mean, black women, wan...   \n",
       "4  [africa, african, chinese, continent, countrie...   \n",
       "5  [brothers sisters, spirit, clay, said, amen, k...   \n",
       "6  [like, kids, family, say, father, mother, age,...   \n",
       "7  [women, shes, traditional, married, feminine, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Some in the community may feel uneasy as we a...  \n",
       "1  [McCurran boys!  Father in the sky, I shed tea...  \n",
       "2  [I told y'all that when I get hot and I get to...  \n",
       "3  [hey yo what up youtube y'all know who it is. ...  \n",
       "4  [Hello.  Welcome to Connecting the Dots on the...  \n",
       "5  [Hi guys, this is your sister Karen Gidden in ...  \n",
       "6  [What's happening, fam?  LA y'all movement sti...  \n",
       "7  [Hello, I'm Nicole Michelle, founder and femin...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    keybert = KeyBERTInspired()\n",
    "    mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert,\n",
    "        \"MMR\": mmr,\n",
    "    }\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e8400-0403-45c2-a8d5-fd5d6e84637f",
   "metadata": {},
   "source": [
    "## all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a357103b-990d-4ba4-aa21-b5c9b8a25d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n",
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n",
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>99</td>\n",
       "      <td>-1_like_don_black_just</td>\n",
       "      <td>[like, don, black, just, people, going, know, ...</td>\n",
       "      <td>[relationship, episode, love, 90, family, vide...</td>\n",
       "      <td>[like, black, just, people, hair, want, time, ...</td>\n",
       "      <td>[hey guys so i wanted to come on here and talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0_police_know_don_just</td>\n",
       "      <td>[police, know, don, just, going, people, like,...</td>\n",
       "      <td>[police, investigation, cops, killed, officer,...</td>\n",
       "      <td>[police, know, like, black, video, look, didn,...</td>\n",
       "      <td>[How did you know they were fleeing?  Good det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>1_high_going_people_know</td>\n",
       "      <td>[high, going, people, know, just, said, shall,...</td>\n",
       "      <td>[prophecy, prophecies, messiah, scriptures, ho...</td>\n",
       "      <td>[high, know, shall, spirit, lord, verse, messi...</td>\n",
       "      <td>[All esteem to the Most High Elohim.  This is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>2_africa_thank_yeah_ghana</td>\n",
       "      <td>[africa, thank, yeah, ghana, like, know, come,...</td>\n",
       "      <td>[come africa, blacksit family, africans, afric...</td>\n",
       "      <td>[africa, ghana, come, nigeria, say, kenya, lov...</td>\n",
       "      <td>[yeah yeah we make home i make homemade ice cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>3_black_folk_black folk_white</td>\n",
       "      <td>[black, folk, black folk, white, people, right...</td>\n",
       "      <td>[civil rights, blacks, slavery, white supremac...</td>\n",
       "      <td>[black, black folk, rights, black people, coun...</td>\n",
       "      <td>[Hey, what's up guys?  I'm Dr.  Boyce Watkins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>4_bible_right_god_read</td>\n",
       "      <td>[bible, right, god, read, verse, man, people, ...</td>\n",
       "      <td>[according bible, deuteronomy, israelites, bib...</td>\n",
       "      <td>[bible, read, verse, come, okay, know, white, ...</td>\n",
       "      <td>[He speaks smooth things to you.  This is what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>5_yeah_shit_man_got</td>\n",
       "      <td>[yeah, shit, man, got, movie, like, blackout, ...</td>\n",
       "      <td>[movie, monday monday monday, ha ha, doing shi...</td>\n",
       "      <td>[man, movie, like, blackout, monday monday, ai...</td>\n",
       "      <td>[yo hello all right we're here.  what's going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6_bethel_congregation_bethelites_circuit</td>\n",
       "      <td>[bethel, congregation, bethelites, circuit, el...</td>\n",
       "      <td>[bethelites, congregation, jehovah witnesses, ...</td>\n",
       "      <td>[congregation, bethelites, elders, jehovah, ov...</td>\n",
       "      <td>[It was really bad.  That was the whole focus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>7_women_men_know_like</td>\n",
       "      <td>[women, men, know, like, chick, pill, black, w...</td>\n",
       "      <td>[documentary, black women, white women, black ...</td>\n",
       "      <td>[women, know, like, pill, porn, black men, jac...</td>\n",
       "      <td>[now available on paperback and kindle unlimit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                      Name  \\\n",
       "0     -1     99                    -1_like_don_black_just   \n",
       "1      0     68                    0_police_know_don_just   \n",
       "2      1     65                  1_high_going_people_know   \n",
       "3      2     50                 2_africa_thank_yeah_ghana   \n",
       "4      3     44             3_black_folk_black folk_white   \n",
       "5      4     37                    4_bible_right_god_read   \n",
       "6      5     33                       5_yeah_shit_man_got   \n",
       "7      6     31  6_bethel_congregation_bethelites_circuit   \n",
       "8      7     27                     7_women_men_know_like   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [like, don, black, just, people, going, know, ...   \n",
       "1  [police, know, don, just, going, people, like,...   \n",
       "2  [high, going, people, know, just, said, shall,...   \n",
       "3  [africa, thank, yeah, ghana, like, know, come,...   \n",
       "4  [black, folk, black folk, white, people, right...   \n",
       "5  [bible, right, god, read, verse, man, people, ...   \n",
       "6  [yeah, shit, man, got, movie, like, blackout, ...   \n",
       "7  [bethel, congregation, bethelites, circuit, el...   \n",
       "8  [women, men, know, like, chick, pill, black, w...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [relationship, episode, love, 90, family, vide...   \n",
       "1  [police, investigation, cops, killed, officer,...   \n",
       "2  [prophecy, prophecies, messiah, scriptures, ho...   \n",
       "3  [come africa, blacksit family, africans, afric...   \n",
       "4  [civil rights, blacks, slavery, white supremac...   \n",
       "5  [according bible, deuteronomy, israelites, bib...   \n",
       "6  [movie, monday monday monday, ha ha, doing shi...   \n",
       "7  [bethelites, congregation, jehovah witnesses, ...   \n",
       "8  [documentary, black women, white women, black ...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [like, black, just, people, hair, want, time, ...   \n",
       "1  [police, know, like, black, video, look, didn,...   \n",
       "2  [high, know, shall, spirit, lord, verse, messi...   \n",
       "3  [africa, ghana, come, nigeria, say, kenya, lov...   \n",
       "4  [black, black folk, rights, black people, coun...   \n",
       "5  [bible, read, verse, come, okay, know, white, ...   \n",
       "6  [man, movie, like, blackout, monday monday, ai...   \n",
       "7  [congregation, bethelites, elders, jehovah, ov...   \n",
       "8  [women, know, like, pill, porn, black men, jac...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [hey guys so i wanted to come on here and talk...  \n",
       "1  [How did you know they were fleeing?  Good det...  \n",
       "2  [All esteem to the Most High Elohim.  This is ...  \n",
       "3  [yeah yeah we make home i make homemade ice cr...  \n",
       "4  [Hey, what's up guys?  I'm Dr.  Boyce Watkins ...  \n",
       "5  [He speaks smooth things to you.  This is what...  \n",
       "6  [yo hello all right we're here.  what's going ...  \n",
       "7  [It was really bad.  That was the whole focus ...  \n",
       "8  [now available on paperback and kindle unlimit...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>113</td>\n",
       "      <td>-1_know_like_just_people</td>\n",
       "      <td>[know, like, just, people, don, got, jehovah, ...</td>\n",
       "      <td>[witness, jehovah witnesses, witnesses, jehova...</td>\n",
       "      <td>[like, people, jehovah, black, family, say, ro...</td>\n",
       "      <td>[Damn, Money Man, let's go.  They already took...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0_lord_right_know_going</td>\n",
       "      <td>[lord, right, know, going, people, high, oh, g...</td>\n",
       "      <td>[judah, israelites, israelite, jews, shalom, s...</td>\n",
       "      <td>[high, god, israel, said, just, lord oh lord, ...</td>\n",
       "      <td>[shalom call hello you.  how about you?  and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1_police_officers_know_going</td>\n",
       "      <td>[police, officers, know, going, got, just, don...</td>\n",
       "      <td>[officer, police, cops, crime, officers, viole...</td>\n",
       "      <td>[police, officers, black, cops, officer, media...</td>\n",
       "      <td>[Welcome to Sergeant Dorsey Speaks.  I'm a ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>2_women_black_men_woman</td>\n",
       "      <td>[women, black, men, woman, black women, white,...</td>\n",
       "      <td>[black women, white women, black men, black wo...</td>\n",
       "      <td>[woman, black women, white, black men, man, wh...</td>\n",
       "      <td>[It's time to talk about housewives and workin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>3_africa_african_france_countries</td>\n",
       "      <td>[africa, african, france, countries, continent...</td>\n",
       "      <td>[african leaders, africa, africans, african co...</td>\n",
       "      <td>[africa, france, countries, continent, african...</td>\n",
       "      <td>[Permit me to annoy you.  In many African nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>4_home home_home home home_purchase_tracks today</td>\n",
       "      <td>[home home, home home home, purchase, tracks t...</td>\n",
       "      <td>[black farmers, farmers, farmer, plots, donkey...</td>\n",
       "      <td>[home, today purchase tracks, donkey, land, ho...</td>\n",
       "      <td>[You don't need any more of those. Yes, depend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>5_covid_know_black folk_masks</td>\n",
       "      <td>[covid, know, black folk, masks, thank, folk, ...</td>\n",
       "      <td>[coronavirus, covid 19, covid, pandemic, quara...</td>\n",
       "      <td>[covid, know, black folk, masks, vaccine, coro...</td>\n",
       "      <td>[What you just heard was an interview by phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>6_black_white_biden_black people</td>\n",
       "      <td>[black, white, biden, black people, people, jo...</td>\n",
       "      <td>[black leaders, care black lives, black commun...</td>\n",
       "      <td>[white, biden, black people, black leaders, sa...</td>\n",
       "      <td>[Lie can go around the world three times befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>7_yeah_bro_like_shit</td>\n",
       "      <td>[yeah, bro, like, shit, nigga, got, car, know,...</td>\n",
       "      <td>[nigga, truck, talking, niggas, dmx, motherfuc...</td>\n",
       "      <td>[like, nigga, car, man, atlanta, son, dmx, tow...</td>\n",
       "      <td>[I got to talk about DMX.  Hold on, let me twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>8_mean_man_yo_people</td>\n",
       "      <td>[mean, man, yo, people, don, shit, ain, day, e...</td>\n",
       "      <td>[racial hatred, racism, man don, nigga, youtub...</td>\n",
       "      <td>[mean, yo, ain, black, black people, videos, l...</td>\n",
       "      <td>[hey yo what up youtube y'all know who it is. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                              Name  \\\n",
       "0     -1    113                          -1_know_like_just_people   \n",
       "1      0    122                           0_lord_right_know_going   \n",
       "2      1     59                      1_police_officers_know_going   \n",
       "3      2     42                           2_women_black_men_woman   \n",
       "4      3     27                 3_africa_african_france_countries   \n",
       "5      4     25  4_home home_home home home_purchase_tracks today   \n",
       "6      5     23                     5_covid_know_black folk_masks   \n",
       "7      6     20                  6_black_white_biden_black people   \n",
       "8      7     17                              7_yeah_bro_like_shit   \n",
       "9      8     16                              8_mean_man_yo_people   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, like, just, people, don, got, jehovah, ...   \n",
       "1  [lord, right, know, going, people, high, oh, g...   \n",
       "2  [police, officers, know, going, got, just, don...   \n",
       "3  [women, black, men, woman, black women, white,...   \n",
       "4  [africa, african, france, countries, continent...   \n",
       "5  [home home, home home home, purchase, tracks t...   \n",
       "6  [covid, know, black folk, masks, thank, folk, ...   \n",
       "7  [black, white, biden, black people, people, jo...   \n",
       "8  [yeah, bro, like, shit, nigga, got, car, know,...   \n",
       "9  [mean, man, yo, people, don, shit, ain, day, e...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [witness, jehovah witnesses, witnesses, jehova...   \n",
       "1  [judah, israelites, israelite, jews, shalom, s...   \n",
       "2  [officer, police, cops, crime, officers, viole...   \n",
       "3  [black women, white women, black men, black wo...   \n",
       "4  [african leaders, africa, africans, african co...   \n",
       "5  [black farmers, farmers, farmer, plots, donkey...   \n",
       "6  [coronavirus, covid 19, covid, pandemic, quara...   \n",
       "7  [black leaders, care black lives, black commun...   \n",
       "8  [nigga, truck, talking, niggas, dmx, motherfuc...   \n",
       "9  [racial hatred, racism, man don, nigga, youtub...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [like, people, jehovah, black, family, say, ro...   \n",
       "1  [high, god, israel, said, just, lord oh lord, ...   \n",
       "2  [police, officers, black, cops, officer, media...   \n",
       "3  [woman, black women, white, black men, man, wh...   \n",
       "4  [africa, france, countries, continent, african...   \n",
       "5  [home, today purchase tracks, donkey, land, ho...   \n",
       "6  [covid, know, black folk, masks, vaccine, coro...   \n",
       "7  [white, biden, black people, black leaders, sa...   \n",
       "8  [like, nigga, car, man, atlanta, son, dmx, tow...   \n",
       "9  [mean, yo, ain, black, black people, videos, l...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Damn, Money Man, let's go.  They already took...  \n",
       "1  [shalom call hello you.  how about you?  and i...  \n",
       "2  [Welcome to Sergeant Dorsey Speaks.  I'm a ret...  \n",
       "3  [It's time to talk about housewives and workin...  \n",
       "4  [Permit me to annoy you.  In many African nati...  \n",
       "5  [You don't need any more of those. Yes, depend...  \n",
       "6  [What you just heard was an interview by phone...  \n",
       "7  [Lie can go around the world three times befor...  \n",
       "8  [I got to talk about DMX.  Hold on, let me twe...  \n",
       "9  [hey yo what up youtube y'all know who it is. ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    keybert = KeyBERTInspired()\n",
    "    mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert,\n",
    "        \"MMR\": mmr,\n",
    "    }\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        embedding_model=\"all-mpnet-base-v2\",\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc4383-148c-4355-99a7-9a5e09740c68",
   "metadata": {},
   "source": [
    "## all-mpnet-base-v2 - SUMMARIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2228a003-e1e9-4982-b890-946345457a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n",
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n",
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>36</td>\n",
       "      <td>-1_youtuber_michael_youtuber discusses_ashley</td>\n",
       "      <td>[youtuber, michael, youtuber discusses, ashley...</td>\n",
       "      <td>[90 day fiancé, fiancé, day fiancé, couples, r...</td>\n",
       "      <td>[michael, youtuber discusses, ashley, reviewer...</td>\n",
       "      <td>[The YouTuber discusses the first half of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>0_black_speaker_people_black people</td>\n",
       "      <td>[black, speaker, people, black people, white, ...</td>\n",
       "      <td>[black community, white supremacy, black men, ...</td>\n",
       "      <td>[black, speaker, people, black people, police,...</td>\n",
       "      <td>[The speaker is discussing the SYSBM movement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>1_speaker_god_people_high</td>\n",
       "      <td>[speaker, god, people, high, bible, book, isra...</td>\n",
       "      <td>[bible, biblical, prophets, prophecy, israelit...</td>\n",
       "      <td>[people, bible, references, israelites, argue,...</td>\n",
       "      <td>[The speaker emphasizes the importance of know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>2_africa_video_ghana_viewers</td>\n",
       "      <td>[africa, video, ghana, viewers, african, gambi...</td>\n",
       "      <td>[documentary, video, growth, africa web tv, ha...</td>\n",
       "      <td>[africa, ghana, viewers, african, gambia, visi...</td>\n",
       "      <td>[The video is a vlog by Juliet, a member of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                           Name  \\\n",
       "0     -1     36  -1_youtuber_michael_youtuber discusses_ashley   \n",
       "1      0    176            0_black_speaker_people_black people   \n",
       "2      1    170                      1_speaker_god_people_high   \n",
       "3      2     72                   2_africa_video_ghana_viewers   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [youtuber, michael, youtuber discusses, ashley...   \n",
       "1  [black, speaker, people, black people, white, ...   \n",
       "2  [speaker, god, people, high, bible, book, isra...   \n",
       "3  [africa, video, ghana, viewers, african, gambi...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [90 day fiancé, fiancé, day fiancé, couples, r...   \n",
       "1  [black community, white supremacy, black men, ...   \n",
       "2  [bible, biblical, prophets, prophecy, israelit...   \n",
       "3  [documentary, video, growth, africa web tv, ha...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [michael, youtuber discusses, ashley, reviewer...   \n",
       "1  [black, speaker, people, black people, police,...   \n",
       "2  [people, bible, references, israelites, argue,...   \n",
       "3  [africa, ghana, viewers, african, gambia, visi...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [The YouTuber discusses the first half of the ...  \n",
       "1  [The speaker is discussing the SYSBM movement,...  \n",
       "2  [The speaker emphasizes the importance of know...  \n",
       "3  [The video is a vlog by Juliet, a member of th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>74</td>\n",
       "      <td>-1_organization_jehovah_witnesses_jehovah witn...</td>\n",
       "      <td>[organization, jehovah, witnesses, jehovah wit...</td>\n",
       "      <td>[encourages listeners, jehovah witnesses, jeho...</td>\n",
       "      <td>[organization, jehovah witnesses, watchtower, ...</td>\n",
       "      <td>[Hello everyone, welcome back to my channel. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>212</td>\n",
       "      <td>0_black_speaker_women_people</td>\n",
       "      <td>[black, speaker, women, people, men, white, co...</td>\n",
       "      <td>[police, violence, racism, officer, black comm...</td>\n",
       "      <td>[speaker, police, black men, video, violence, ...</td>\n",
       "      <td>[A disturbing incident occurred in a Walmart p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>1_speaker_god_people_israelites</td>\n",
       "      <td>[speaker, god, people, israelites, judah, bibl...</td>\n",
       "      <td>[israelites, israelite, gentiles, jews, big ju...</td>\n",
       "      <td>[israelites, judah, bible, israel, biblical, h...</td>\n",
       "      <td>[Dante Fortson is a YouTube creator who challe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2_africa_african_countries_africans</td>\n",
       "      <td>[africa, african, countries, africans, speaker...</td>\n",
       "      <td>[african leaders, african nations, africa, afr...</td>\n",
       "      <td>[africa, african, africans, gambia, developmen...</td>\n",
       "      <td>[African leaders have made statements that lef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3_state_covid_covid 19_cross river</td>\n",
       "      <td>[state, covid, covid 19, cross river, river st...</td>\n",
       "      <td>[covid 19, pandemic, covid 19 dr, quarantine, ...</td>\n",
       "      <td>[covid, covid 19, cross river state, vaccine, ...</td>\n",
       "      <td>[The Blacksit family has expanded, and the fat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  \\\n",
       "0     -1     74  -1_organization_jehovah_witnesses_jehovah witn...   \n",
       "1      0    212                       0_black_speaker_women_people   \n",
       "2      1    124                    1_speaker_god_people_israelites   \n",
       "3      2     41                2_africa_african_countries_africans   \n",
       "4      3     13                 3_state_covid_covid 19_cross river   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [organization, jehovah, witnesses, jehovah wit...   \n",
       "1  [black, speaker, women, people, men, white, co...   \n",
       "2  [speaker, god, people, israelites, judah, bibl...   \n",
       "3  [africa, african, countries, africans, speaker...   \n",
       "4  [state, covid, covid 19, cross river, river st...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [encourages listeners, jehovah witnesses, jeho...   \n",
       "1  [police, violence, racism, officer, black comm...   \n",
       "2  [israelites, israelite, gentiles, jews, big ju...   \n",
       "3  [african leaders, african nations, africa, afr...   \n",
       "4  [covid 19, pandemic, covid 19 dr, quarantine, ...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [organization, jehovah witnesses, watchtower, ...   \n",
       "1  [speaker, police, black men, video, violence, ...   \n",
       "2  [israelites, judah, bible, israel, biblical, h...   \n",
       "3  [africa, african, africans, gambia, developmen...   \n",
       "4  [covid, covid 19, cross river state, vaccine, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Hello everyone, welcome back to my channel. T...  \n",
       "1  [A disturbing incident occurred in a Walmart p...  \n",
       "2  [Dante Fortson is a YouTube creator who challe...  \n",
       "3  [African leaders have made statements that lef...  \n",
       "4  [The Blacksit family has expanded, and the fat...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['summary'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    keybert = KeyBERTInspired()\n",
    "    mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert,\n",
    "        \"MMR\": mmr,\n",
    "    }\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        embedding_model=\"all-mpnet-base-v2\",\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before_summarized, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after_summarized, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fbe8d-d63d-4a0b-a32c-3d466c42f99b",
   "metadata": {},
   "source": [
    "## longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32236a0e-4581-4710-a46f-a8b7c2fbc1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 8, Truncated Length: 8\n",
      "Original Sequence: This is a short sentence.\n",
      "Truncated Sequence: This is a short sentence.\n",
      "Truncated correctly: True\n",
      "\n",
      "Original Length: 6253, Truncated Length: 4096\n",
      "Original Sequence: This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. \n",
      "Truncated Sequence: This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length\n",
      "Truncated correctly: True\n",
      "\n",
      "Original Length: 6, Truncated Length: 6\n",
      "Original Sequence: Another short one.\n",
      "Truncated Sequence: Another short one.\n",
      "Truncated correctly: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "def truncate_sequences(sequences, max_length, tokenizer):\n",
    "    truncated_sequences = []\n",
    "    for seq in sequences:\n",
    "        tokenized = tokenizer(seq, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        truncated_seq = tokenizer.decode(tokenized.input_ids[0], skip_special_tokens=True)\n",
    "        truncated_sequences.append(truncated_seq)\n",
    "    return truncated_sequences\n",
    "\n",
    "def verify_truncation_logic(sequences, max_length, tokenizer):\n",
    "    for seq in sequences:\n",
    "        tokenized = tokenizer(seq, return_tensors='pt')\n",
    "        original_length = len(tokenized.input_ids[0])\n",
    "        \n",
    "        truncated_seq = truncate_sequences([seq], max_length, tokenizer)[0]\n",
    "        truncated_tokenized = tokenizer(truncated_seq, return_tensors='pt')\n",
    "        truncated_length = len(truncated_tokenized.input_ids[0])\n",
    "        \n",
    "        print(f\"Original Length: {original_length}, Truncated Length: {truncated_length}\")\n",
    "        print(f\"Original Sequence: {seq}\")\n",
    "        print(f\"Truncated Sequence: {truncated_seq}\")\n",
    "        print(f\"Truncated correctly: {truncated_length <= max_length}\\n\")\n",
    "\n",
    "# Example sequences\n",
    "sequences = [\n",
    "    \"This is a short sentence.\",\n",
    "    \"This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. \" * 250, # artificially long sequence\n",
    "    \"Another short one.\"\n",
    "]\n",
    "\n",
    "max_length = 4096\n",
    "verify_truncation_logic(sequences, max_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e796981a-1e71-4413-981c-e5622dfc6d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/longformer-base-4096. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for ideology: BLACK with 454 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/longformer-base-4096. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>0_know_people_going_like</td>\n",
       "      <td>[know, people, going, like, don, just, right, ...</td>\n",
       "      <td>[black people, everybody, believe, trying, say...</td>\n",
       "      <td>[know, people, just, got, come, think, okay, r...</td>\n",
       "      <td>[This is your brother Malcolm coming at you wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1_jesus jesus_thank_conference_jesus jesus jesus</td>\n",
       "      <td>[jesus jesus, thank, conference, jesus jesus j...</td>\n",
       "      <td>[miami international airport, embassy suites h...</td>\n",
       "      <td>[jesus jesus, jesus jesus jesus, jesus, krispy...</td>\n",
       "      <td>[Hello, family.  Thank you for your continued ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                              Name  \\\n",
       "0      0    426                          0_know_people_going_like   \n",
       "1      1     28  1_jesus jesus_thank_conference_jesus jesus jesus   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, people, going, like, don, just, right, ...   \n",
       "1  [jesus jesus, thank, conference, jesus jesus j...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [black people, everybody, believe, trying, say...   \n",
       "1  [miami international airport, embassy suites h...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [know, people, just, got, come, think, okay, r...   \n",
       "1  [jesus jesus, jesus jesus jesus, jesus, krispy...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [This is your brother Malcolm coming at you wi...  \n",
       "1  [Hello, family.  Thank you for your continued ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>0_know_people_going_like</td>\n",
       "      <td>[know, people, going, like, don, just, black, ...</td>\n",
       "      <td>[black people, everybody, don want, believe, t...</td>\n",
       "      <td>[know, people, just, come, think, okay, oh, ye...</td>\n",
       "      <td>[Good rising, brethren.  This is Big Judah, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1_thank_chardian_pearson_light</td>\n",
       "      <td>[thank, chardian, pearson, light, carlton, car...</td>\n",
       "      <td>[informative educational videos, suggestions n...</td>\n",
       "      <td>[carlton, carlton pearson, triggers, steal awa...</td>\n",
       "      <td>[Hello, I'm Carlton Pearson.  Listen to this w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                            Name  \\\n",
       "0      0    417        0_know_people_going_like   \n",
       "1      1     47  1_thank_chardian_pearson_light   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, people, going, like, don, just, black, ...   \n",
       "1  [thank, chardian, pearson, light, carlton, car...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [black people, everybody, don want, believe, t...   \n",
       "1  [informative educational videos, suggestions n...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [know, people, just, come, think, okay, oh, ye...   \n",
       "1  [carlton, carlton pearson, triggers, steal awa...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Good rising, brethren.  This is Big Judah, co...  \n",
       "1  [Hello, I'm Carlton Pearson.  Listen to this w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "def truncate_sequences(sequences, max_length):\n",
    "    truncated_sequences = []\n",
    "    for seq in sequences:\n",
    "        tokenized = tokenizer(seq, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        truncated_sequences.append(tokenizer.decode(tokenized.input_ids[0], skip_special_tokens=True))\n",
    "    return truncated_sequences\n",
    "\n",
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    ideology_texts = truncate_sequences(ideology_texts, 4096)\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    keybert = KeyBERTInspired()\n",
    "    mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert,\n",
    "        \"MMR\": mmr,\n",
    "    }\n",
    "    topic_model = BERTopic(\n",
    "        # verbose=True\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model,\n",
    "        embedding_model=\"allenai/longformer-base-4096\",\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts)\n",
    "    return topic_model, topics, probs\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology)\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology)\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n",
    "\n",
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
