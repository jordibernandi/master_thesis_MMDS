{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56127460-5435-4ff9-9ea7-ea320151ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac4cf4c7b0b4ddb8ae0df48082dadcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import regex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=4\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "# model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "model_llm = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_llm)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_llm,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=device,\n",
    "    do_sample=False,  # No sampling\n",
    "    temperature=None,  # Not needed when do_sample=False\n",
    "    top_p=None,        # Not needed when do_sample=False\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure proper padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7913983-a9aa-4ca0-b9d3-39580cc5e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder containing the files\n",
    "folder_path = \"../Llama3_1_Model_2/topicGPT/data/output/FINAL\"\n",
    "\n",
    "# List of unique ideologies\n",
    "ideologies = [\"ANTI_SJW\", \"ANTI_THEIST\", \"BLACK\", \"CONSPIRACY\", \"LGBT\", \"LIBERTARIAN\", \n",
    "              \"MRA\", \"PARTISAN_RIGHT\", \"PARTISAN_LEFT\", \"QANON\", \"RELIGIOUS_CONSERVATIVE\", \n",
    "              \"SOCIAL_JUSTICE\", \"SOCIALIST\", \"WHITE_IDENTITARIAN\"]\n",
    "# ideologies = [\"ANTI_SJW\"]\n",
    "\n",
    "def remove_parenthesized_phrases(text):\n",
    "    # Use regex to find and remove all substrings that start with '(' and end with ')'\n",
    "    cleaned_text = re.sub(r'\\s*\\(.*?\\)', '', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def doc_label(df, topics_list):\n",
    "    \"\"\"\n",
    "    Add labels to each document based on the topics generated for it.\n",
    "    - df: dataframe of documents\n",
    "    - topics_list: list of topics\n",
    "    \"\"\"\n",
    "    pattern = regex.compile(\"^\\[(\\d+)\\] ([\\w\\s]+):(.+)\")\n",
    "    all_topics = []\n",
    "    for line in df[\"responses\"].tolist():\n",
    "        if type(line) == str:\n",
    "            line = line.split(\"\\n\")\n",
    "            line_topics = []\n",
    "            for topic in line:\n",
    "                if regex.match(pattern, topic):\n",
    "                    groups = regex.match(pattern, topic).groups()\n",
    "                    lvl, name, desc = int(groups[0]), groups[1], groups[2]\n",
    "                    if f\"[{lvl}] {name}\" in topics_list:\n",
    "                        line_topics.append(remove_parenthesized_phrases(desc))\n",
    "                        # line_topics.append(topic)\n",
    "            line_topics = list(set(line_topics))\n",
    "            if len(line_topics) > 0:\n",
    "                all_topics.append(line_topics)\n",
    "            else:\n",
    "                all_topics.append([\"None\"])\n",
    "        else:\n",
    "            all_topics.append([\"None\"])\n",
    "    return all_topics\n",
    "\n",
    "# Function to extract main topic and its subtopics dynamically based on a specified topic name\n",
    "def extract_topic_data_per_ideology_period(file_path, main_topic=\"Health\"):\n",
    "    try:\n",
    "        df = pd.read_json(str(file_path), lines=True)\n",
    "        topics_list = [f\"[1] {main_topic}\"]\n",
    "        df[\"topics\"] = doc_label(df, topics_list)\n",
    "        # Excluding rows with more than one unique topic//\"None\" ----\n",
    "        df[\"num_topics\"] = df[\"topics\"].apply(lambda x: len(set(x)))\n",
    "        df = df[df[\"topics\"].apply(lambda x: x != [\"None\"])].reset_index(drop=True)\n",
    "\n",
    "        all_embeddings = []\n",
    "        concatinated_topics = \"\"\n",
    "    \n",
    "        for index, row in df.iterrows():\n",
    "            # Step 1: Concatenate all sentences in the 'topics' column for this row\n",
    "            concatenated_text = ' '.join(row['topics'])  # Join all sentences in the list\n",
    "            # print(\"concatenated_text\", concatenated_text)\n",
    "            # Step 2: Generate embedding for the concatenated text\n",
    "            embedding = model.encode(concatenated_text, convert_to_tensor=True)  # This will be on the GPU (cuda:0)\n",
    "            all_embeddings.append(embedding.cpu())  # Move to CPU immediately and append to list\n",
    "            concatinated_topics = concatinated_topics + \" - \" + concatenated_text\n",
    "        \n",
    "        # Step 3: Stack all embeddings and compute the average embedding\n",
    "        stacked_embeddings = torch.stack(all_embeddings)  # Now all embeddings are on the CPU\n",
    "        average_embedding = torch.mean(stacked_embeddings, dim=0)  # Compute the mean across all rows\n",
    "\n",
    "        return {\n",
    "            \"average_embedding\": average_embedding,\n",
    "            \"concatinated_topics\": concatinated_topics,\n",
    "            \"freq\": len(df.index)\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return {}\n",
    "\n",
    "def generate_text(prompt, max_tokens):\n",
    "    \"\"\"\n",
    "    Generate text using a local model with transformers.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are equipped with advanced analytical techniques. Your goal is to distill complex information from various topics and summarize the overall subject being discussed in 1 paragraph. Return only the results, without any additional comments.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the following list of topics:\\n\\n{prompt}\"},\n",
    "    ]\n",
    "    \n",
    "    outputs = generator(\n",
    "        messages,\n",
    "        max_new_tokens=max_tokens,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "def generate_graph(main_topic):\n",
    "    # DataFrame to hold extracted data\n",
    "    data = []\n",
    "    \n",
    "    # Main topic to analyze (can be changed dynamically)\n",
    "    main_topic = main_topic\n",
    "    \n",
    "    # Iterate over unique ideologies and time phases\n",
    "    for ideology in ideologies:\n",
    "        for time_phase in [\"before\", \"after\"]:  # Check both after and before files\n",
    "            \n",
    "            # Construct markdown filename\n",
    "            filename = f\"assignment_{time_phase}_{ideology}_FULL.jsonl\"\n",
    "            file_path = os.path.join(folder_path, filename)    \n",
    "\n",
    "            topic_data = extract_topic_data_per_ideology_period(file_path, main_topic)\n",
    "\n",
    "            summarized = generate_text(topic_data[\"concatinated_topics\"], 384)\n",
    "\n",
    "            print(\"TOPIC: \", main_topic)\n",
    "            print(\"IDEOLOGY: \", ideology)\n",
    "            print(\"TIME PHASE: \", time_phase)\n",
    "            print(\"SUMMARIZED: \", summarized)\n",
    "            print(\"\\n--------------------------------------------------------------------\\n\")\n",
    "            # print(topic_data[\"concatinated_topics\"])\n",
    "\n",
    "            summarized_embedding = model.encode(summarized, convert_to_tensor=True)  # This will be on the GPU (cuda:0)\n",
    "\n",
    "            \n",
    "            # Append the data\n",
    "            data.append({\n",
    "                \"ideology\": ideology,\n",
    "                \"period\": time_phase,\n",
    "                # \"embedding\": topic_data[\"average_embedding\"].cpu().numpy(),\n",
    "                \"embedding\": summarized_embedding.cpu().numpy(),\n",
    "            })\n",
    "    \n",
    "    # Convert data into a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Extract embeddings and labels\n",
    "    X = np.array(df['embedding'].tolist())  # Embeddings\n",
    "    y_ideology = df['ideology']  # Ideology\n",
    "    y_period = df['period']  # Period (before/after)\n",
    "    \n",
    "    # Label encode the ideologies for visualization\n",
    "    le_ideology = LabelEncoder()\n",
    "    y_ideology_encoded = le_ideology.fit_transform(y_ideology)\n",
    "    \n",
    "    # Apply UMAP to project the embeddings into a lower-dimensional space (2D)\n",
    "    umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap = umap_model.fit_transform(X)\n",
    "    \n",
    "    # Plot the UMAP projection to visualize ideologies with labels\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = {'before': 'blue', 'after': 'orange'}\n",
    "    markers = {'before': 'o', 'after': 's'}\n",
    "    \n",
    "    # To track whether we have added a label for each period\n",
    "    label_added = {'before': False, 'after': False}\n",
    "    \n",
    "    for i, (x, y, ideology, period) in enumerate(zip(X_umap[:, 0], X_umap[:, 1], df['ideology'], df['period'])):\n",
    "        # Add label for the legend only once per period\n",
    "        if not label_added[period]:\n",
    "            plt.scatter(x, y, color=colors[period], marker=markers[period], label=f'Period: {period}', alpha=0.7)\n",
    "            label_added[period] = True\n",
    "        else:\n",
    "            plt.scatter(x, y, color=colors[period], marker=markers[period], alpha=0.7)\n",
    "        \n",
    "        # Annotate the points with ideology\n",
    "        plt.annotate(ideology, (x, y), fontsize=9, alpha=0.8)\n",
    "    \n",
    "    # Set up the plot details\n",
    "    plt.title(\"UMAP Projection of Ideologies (Before and After COVID)\")\n",
    "    plt.xlabel(\"UMAP Component 1\")\n",
    "    plt.ylabel(\"UMAP Component 2\")\n",
    "    plt.legend(loc='best')  # Legend showing both periods\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # # Analyze cosine similarity between ideologies before and after COVID\n",
    "    # similarities = {}\n",
    "    # for ideology in le_ideology.classes_:\n",
    "    #     before_emb = X_umap[(df['ideology'] == ideology) & (df['period'] == 'before')]\n",
    "    #     after_emb = X_umap[(df['ideology'] == ideology) & (df['period'] == 'after')]\n",
    "        \n",
    "    #     if before_emb.shape[0] > 0 and after_emb.shape[0] > 0:\n",
    "    #         # Calculate cosine similarity between embeddings before and after COVID for each ideology\n",
    "    #         sim = cosine_similarity(before_emb.mean(axis=0).reshape(1, -1), \n",
    "    #                                 after_emb.mean(axis=0).reshape(1, -1))[0][0]\n",
    "    #         similarities[ideology] = sim\n",
    "    \n",
    "    # # Output the cosine similarity results\n",
    "    # print(f\"Cosine Similarity between Ideologies Before and After COVID ({main_topic} only):\")\n",
    "    # for ideology, similarity in similarities.items():\n",
    "    #     print(f\"{ideology}: {similarity:.4f}\")\n",
    "    \n",
    "    # Analyze cosine similarity between ideologies before and after COVID\n",
    "    similarities = {}\n",
    "    for ideology in le_ideology.classes_:\n",
    "        before_emb = X[(df['ideology'] == ideology) & (df['period'] == 'before')]\n",
    "        after_emb = X[(df['ideology'] == ideology) & (df['period'] == 'after')]\n",
    "    \n",
    "        if before_emb.shape[0] > 0 and after_emb.shape[0] > 0:\n",
    "            # Calculate cosine similarity between embeddings before and after COVID for each ideology\n",
    "            sim = cosine_similarity(before_emb.mean(axis=0).reshape(1, -1), \n",
    "                                    after_emb.mean(axis=0).reshape(1, -1))[0][0]\n",
    "            similarities[ideology] = sim\n",
    "    \n",
    "    # Output the cosine similarity results\n",
    "    print(f\"Cosine Similarity between Ideologies Before and After COVID ({main_topic} only):\")\n",
    "    for ideology, similarity in similarities.items():\n",
    "        print(f\"{ideology}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6aad11-a36c-4a6c-919a-69f184a69837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2158115/3389980170.py:46: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(str(file_path), lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m topics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolitics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGovernment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mgenerate_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 109\u001b[0m, in \u001b[0;36mgenerate_graph\u001b[0;34m(main_topic)\u001b[0m\n\u001b[1;32m    106\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massignment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_phase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mideology\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)    \n\u001b[0;32m--> 109\u001b[0m topic_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_topic_data_per_ideology_period\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m summarized \u001b[38;5;241m=\u001b[39m generate_text(topic_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatinated_topics\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m384\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOPIC: \u001b[39m\u001b[38;5;124m\"\u001b[39m, main_topic)\n",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m, in \u001b[0;36mextract_topic_data_per_ideology_period\u001b[0;34m(file_path, main_topic)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_topic_data_per_ideology_period\u001b[39m(file_path, main_topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHealth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         topics_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[1] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_topic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     48\u001b[0m         df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m doc_label(df, topics_list)\n",
      "File \u001b[0;32m/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/pandas/io/json/_json.py:1023\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1022\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1409\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "topics = [\n",
    "    \"Politics\", \n",
    "    \"Government\", \n",
    "    \"Community\", \n",
    "    \"Human Rights\", \n",
    "    \"Identity\", \n",
    "    \"Social Justice\", \n",
    "    \"Culture\", \n",
    "    \"Human Behavior\", \n",
    "    \"Education\", \n",
    "    \"Relationships\", \n",
    "    \"Personal Growth\", \n",
    "    \"Society\",  \n",
    "    \"Health\",\n",
    "    \"Economy\",\n",
    "    \"Law Enforcement\",\n",
    "    \"Social Commentary\",\n",
    "    \"Media\",\n",
    "    \"Faith\",\n",
    "    \"Leadership\",\n",
    "    \"History\"\n",
    "]\n",
    "\n",
    "for t in topics:\n",
    "    generate_graph(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
