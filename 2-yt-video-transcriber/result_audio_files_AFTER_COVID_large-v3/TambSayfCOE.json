{
    "yt_video_id": "TambSayfCOE",
    "transcripts": [
        {
            "start_time": "",
            "end_time": "",
            "text": "The 2016 election came as a shock to many news watchers.  Times Square has never been quieter right now.  The day before the election, much of the media seemed to be convinced that a Hillary Clinton victory was imminent. ",
            "speaker": {
                "name": "Speaker 0"
            }
        },
        {
            "start_time": "00:00:11,460",
            "end_time": "00:00:16,160",
            "text": "You have to say advantage Clinton as we head into these final hours and this big national lead is why. ",
            "speaker": {
                "name": "Speaker 1"
            }
        },
        {
            "start_time": "00:00:16,440",
            "end_time": "00:00:22,020",
            "text": "And while the polls indicated that she was the favored candidate, we all know how that story ended.  Now that we're approaching another election, both the Trump and Biden campaigns are playing on messages that the polling may not be accurate.  So you may be thinking, how can we ever trust the polls again?  Didn't they get 2016 wrong?  Well, only kind of.  I'm Jamil Mehta, and I'm here to explain to you why you shouldn't give up on polling.  So what happened with the polls in 2016, and what does this mean for 2020?  Before we answer that question, let's take a look at what a poll even is.  A poll is not a crystal ball that tells you who's going to win the election, but rather a survey taken of a sample of voters that lets us guess at how a population as a whole might vote.  Conducting a poll is a bit like grabbing a handful of jelly beans out of a giant jar.  If you grab a random handful, you can estimate the composition of the whole jar from the ones in your hand.  Let's take, for example, this poll by Marist College conducted about a week before election day in 2016.  After calling 940 randomly selected voters, Marist found that Clinton led the popular vote by roughly two points.  And I say roughly, because as you might imagine, no one can be certain that 940 people speak for the entire country.  There is always going to be a margin of error.  The fewer people in your sample, the bigger your margin of error will be.  The larger your sample, the smaller the margin of error will be.  Now this Marist poll, it ended up doing pretty well.  Although Trump won the electoral college, Clinton won the popular vote by just over two percentage points in 2016, about the same margin the poll predicted.  But other polls from the week leading up to the election didn't do quite as well.  So the polls are all over the place.  They're meaningless, right?  Wrong!  One of the key insights we have when doing political analysis at FiveThirtyEight is that looking at all the polls will always give you a better picture of a race than looking at one poll alone.  Here are all the polls in the three weeks leading up to election day.  If we take the average of these polls, you can see that together, the polls did a pretty good job at estimating what the national popular vote would be.  Since 2000, national polls have gotten the popular vote right within about 4 percentage points, and they weren't any more off in 2016 than they've been in previous years.  But we don't elect presidents by national popular vote.  In this country, we have 50 distinct state elections.  That's why polls done to measure how a specific state will vote are much more useful in helping us forecast who will win the electoral college and actually become president.  And that is where the story of 2016 becomes a little more complicated.  In general, state-level polls are more error-prone than national-level polls.  In any given year, most of the state-level polling error tends to favor the same candidate.  For example, in 2012, most state-level polls tended to underestimate Obama's chances, while in 2016, state-level polls were more likely to discount Trump's chances.  But while the state-level polls were off by about 3.4 percentage points on average in 2012, they were off by about 5.4 percentage points in 2016.  So why were the state-level polls more off in 2016?  As you might imagine, there are a few reasons.  For example, there's evidence that more people who told pollsters that they were undecided ended up casting a ballot for Trump.  But one big reason comes down to how pollsters did something called weighting their responses.  Let's return to our jar of jelly beans.  There's always a chance that when you grab a handful, you randomly pull fewer green jelly beans than there were in the jar as a whole.  Pollsters deal with this by weighting their responses.  They know what the US population looks like from census data, which is supposed to count every single American.  That Marist poll we discussed, for example, adjusts the responses by age, gender, income, race, and region to ensure the sample will be representative of the demographics of the American population.  Basically, there aren't enough green jelly beans in your sample, the political opinions of the green jelly beans that you do have should be worth more.  If you're trying to estimate the political opinions of a jar of jelly beans.  But in 2016, there was one factor most pollsters didn't take into account while they were waiting.  Education.  And that turned out to be a more important indicator of who people would vote for in 2016 than in past elections.  Less educated voters were much more likely to vote for Trump.  An analysis of the polling data after the election indicated that they were likely undercounted by the polls.  So that's one big reason why the polls were more off in 2016 than in past years.  So are the polls meaningless then?  It seems like they can't predict who will win.  Whoa, let's not overreact here.  Sure, the state-level polls were a little more off in 2016 than they had been in the recent past.  But remember, this was a very tight race.  In Michigan, the average of the polls was roughly 5 percentage points more Democratic-leading than the actual result turned out to be.  But Trump won that state by just one fifth of 1%.  That was also a big part of why this election was so hard to predict.  That said, the week before the election, FiveThirtyEight's forecast gave Trump about a 3 in 10 chance of winning.  We definitely had him pinned as an underdog based on the polling.  But even based on these imperfect polls, we knew a Trump victory was a real possibility.  Wait, so what does this all mean for 2020?  Great question.  Well, some, but not all pollsters have started to wait for education.  And polling shows that there may be fewer undecided voters this year.  But 2020 is a weird year.  With a global pandemic, mass protests, and an economic crisis, a lot of things are going on that could add more uncertainty than usual.  That's why this year, 538's election forecast has a brand new indicator.  As usual, we're synthesizing national and state-level polls, taking into account the ways that state-level polls tend to be correlated.  And we're considering non-polling factors like how the economy is doing.  But now, we're also accounting for the additional uncertainty that comes from a year with so much news.  So the next time someone tells you that the polls are all wrong or they can't be trusted, send them this video and remind them that they are in fact useful but imperfect estimators.  And tell them that the polls tend to accurately predict the results in the aggregate, but only within plus or minus three percentage points nationally and roughly plus or minus four percentage points statewide.  And don't forget to mention the margin of error and how close the selection was.  and how all the polls are better to look at than one particular poll.  Hey, Dremel again.  If you have any questions, let me know in the comments.  And if you like this video, subscribe to FiveThirtyEight on YouTube. ",
            "speaker": {
                "name": "Speaker 0"
            }
        }
    ]
}