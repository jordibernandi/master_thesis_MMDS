{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af01e61f-3354-43ef-9e34-cfb0751c975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/iharsawi/miniconda3/envs/thesis7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "# model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fb7b29-d28f-4e6a-b039-59ec4f4fc15c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m md_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_phase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mideology\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1TOPIC_ASS_politics.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m md_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, md_filename)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subtopic, count, description \u001b[38;5;129;01min\u001b[39;00m \u001b[43mextract_subtopics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmd_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_topic\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     67\u001b[0m     time_phases\u001b[38;5;241m.\u001b[39mappend(time_phase)\n\u001b[1;32m     68\u001b[0m     all_topics\u001b[38;5;241m.\u001b[39mappend(subtopic)\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mextract_subtopics\u001b[0;34m(file_path, main_topic, subtopic_limit)\u001b[0m\n\u001b[1;32m     41\u001b[0m     subtopic_counts \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mint\u001b[39m(count) \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m subtopic, count, description \u001b[38;5;129;01min\u001b[39;00m subtopics]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Return a structured dictionary with main topic, description, and subtopics\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     45\u001b[0m         subtopics\n\u001b[1;32m     46\u001b[0m     }\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Set the folder containing the files\n",
    "folder_path = \"../Llama3_1_Model_2/topicGPT/data/output/FINAL_SUMMARY\"\n",
    "\n",
    "# List of unique ideologies\n",
    "# ideologies = [\"ANTI_SJW\", \"ANTI_THEIST\", \"BLACK\", \"CONSPIRACY\", \"LGBT\", \"LIBERTARIAN\", \n",
    "#               \"MRA\", \"PARTISAN_RIGHT\", \"PARTISAN_LEFT\", \"QANON\", \"RELIGIOUS_CONSERVATIVE\", \n",
    "#               \"SOCIAL_JUSTICE\", \"SOCIALIST\", \"WHITE_IDENTITARIAN\"]\n",
    "ideologies = [\"ANTI_SJW\"]\n",
    "\n",
    "def extract_subtopics(file_path, main_topic=\"Politics\", subtopic_limit=None):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "\n",
    "        # Regex to match the main topic and extract its description\n",
    "        main_topic_pattern = re.compile(rf'\\[1\\] {main_topic} \\(Count: \\d+\\): (.*?)\\n')\n",
    "        \n",
    "        # Updated regex pattern to capture subtopic, count (optional), and description\n",
    "        subtopic_pattern = re.compile(r'\\[2\\] ([^()]*?)(?: \\(Count:\\s*(\\d+)\\))?: (.*?)\\n')\n",
    "\n",
    "        # Find the section for the specified main topic\n",
    "        main_topic_match = main_topic_pattern.search(data)\n",
    "        if not main_topic_match:\n",
    "            return f\"Main topic '{main_topic}' not found.\"\n",
    "\n",
    "        # Find the position of the main topic\n",
    "        start_pos = main_topic_match.end()\n",
    "        \n",
    "        # Find the next main topic or the end of the string\n",
    "        next_main_topic = re.search(r'\\[1\\]', data[start_pos:])\n",
    "        end_pos = start_pos + (next_main_topic.start() if next_main_topic else len(data))\n",
    "\n",
    "        # Extract the section related to the main topic\n",
    "        section = data[start_pos:end_pos]\n",
    "\n",
    "        # Extract all subtopics, their counts, and their descriptions\n",
    "        subtopics = subtopic_pattern.findall(section)\n",
    "\n",
    "        # Filter out subtopics with no count (count is None or empty)\n",
    "        all_subtopics = [(subtopic.strip()) for subtopic, count, description in subtopics]\n",
    "        subtopic_counts = [(int(count) if count else 0) for subtopic, count, description in subtopics]\n",
    "                \n",
    "        # Return a structured dictionary with main topic, description, and subtopics\n",
    "        return subtopics\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return {}\n",
    "\n",
    "# Main topic to analyze (can be changed dynamically)\n",
    "main_topic = \"Politics\"\n",
    "\n",
    "# Iterate over unique ideologies and time phases\n",
    "for ideology in ideologies:\n",
    "    # Initialize lists for storing topics, their counts, and their ideology\n",
    "    time_phases = []\n",
    "    all_topics = []\n",
    "    topic_counts = []\n",
    "    \n",
    "    for time_phase in [\"after\", \"before\"]:  # Check both after and before files\n",
    "        # Construct markdown filename\n",
    "        md_filename = f\"generation_2_{time_phase}_{ideology}_1TOPIC_ASS_politics.md\"\n",
    "        md_file_path = os.path.join(folder_path, md_filename)\n",
    "        \n",
    "        for subtopic, count, description in extract_subtopics(md_file_path, main_topic):\n",
    "            time_phases.append(time_phase)\n",
    "            all_topics.append(subtopic)\n",
    "            topic_counts.append(count)\n",
    "\n",
    "\n",
    "    consider_count = True  # Set to True to consider counts, or False to ignore them\n",
    "    \n",
    "    # Generate semantic embeddings for the topics\n",
    "    X = model.encode(all_topics)\n",
    "    \n",
    "    if consider_count:\n",
    "        # Multiply embeddings by the square root of the counts to give weight to more frequent topics\n",
    "        weighted_embeddings = np.array([embedding * np.sqrt(count) for embedding, count in zip(X, topic_counts)])\n",
    "    else:\n",
    "        # Use embeddings as-is, without considering topic counts\n",
    "        weighted_embeddings = X\n",
    "    \n",
    "    # Perform UMAP dimensionality reduction\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    embeddings = reducer.fit_transform(weighted_embeddings)\n",
    "    \n",
    "    # Cluster the topics using KMeans\n",
    "    n_clusters = 5  # You can adjust the number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Get the cluster centers for labeling\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Plotting the result\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for time_phase in set(time_phases):\n",
    "        indices = [i for i, ideology in enumerate(time_phases) if ideology == time_phase]\n",
    "        plt.scatter(embeddings[indices, 0], embeddings[indices, 1], label=time_phase, alpha=0.7)\n",
    "    \n",
    "    # Label each cluster with the most common topic in the cluster\n",
    "    for cluster_idx in range(n_clusters):\n",
    "        # Get the indices of topics in this cluster\n",
    "        cluster_topic_indices = np.where(labels == cluster_idx)[0]\n",
    "        \n",
    "        # Get the most common topic in the cluster\n",
    "        cluster_topics = [all_topics[i] for i in cluster_topic_indices]\n",
    "        most_common_topic = max(set(cluster_topics), key=cluster_topics.count)\n",
    "        \n",
    "        # Plot the label at the cluster center\n",
    "        plt.text(cluster_centers[cluster_idx, 0], cluster_centers[cluster_idx, 1], \n",
    "                 most_common_topic, fontsize=12, ha='center', color='black')\n",
    "    \n",
    "    plt.title(\"Topic Analysis Across Ideologies with Cluster Labels\")\n",
    "    plt.legend(loc='best', fontsize='small')\n",
    "    plt.xlabel(\"UMAP Dimension 1\")\n",
    "    plt.ylabel(\"UMAP Dimension 2\")\n",
    "    plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c54007-2f54-4d9e-a37d-5186beabcb58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
