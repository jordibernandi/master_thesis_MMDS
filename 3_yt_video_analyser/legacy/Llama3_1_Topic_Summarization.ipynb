{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79678e7-897c-492c-b8c2-de0efc2738ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b058656b-6f9d-459f-9a6a-543e25c6eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fd50952bc34ea091b96e00ac83a0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23970ff1-9313-48b4-91c0-bc275c779110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8a9cb9-f4a3-4711-99cd-1862e27a563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_x ='''\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful, respectful, and honest assistant for performing topic modeling. Your task is to generate a concise topic for each transcripts provided.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Summarize and merge the following list of topics into {Fixed_Number} of final topics.\n",
    "Consider the frequency of each topic when determining the final list.\n",
    "Only return the name of the top {Fixed_Number} topics, without any explanations and subtopics.\n",
    "The desired output format:\n",
    "Topic 1 : name (frequency)\n",
    "Topic 2 : name (frequency)\n",
    "Topic 3 : name (frequency)\n",
    "The list of topics with their frequencies:\n",
    "{Topic_list}\n",
    "Topics:\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a26f6e-932b-40bb-9155-f2a22ddd88be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc9a2a42be647e3ab2e7e1fd1829b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def process_csv_file(filepath, fixed_numbers):\n",
    "    print(f\"Processing {filepath}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    # Combine topics and their frequencies into a single string\n",
    "    topic_list = ', '.join(f\"{row['Topics']} (frequency: {row['Count']})\" for _, row in data.iterrows())\n",
    "            \n",
    "    formatted_prompt = prompt_x.format(Fixed_Number=fixed_numbers, Topic_list=topic_list)\n",
    "    \n",
    "    content = formatted_prompt\n",
    "    sequences = pipeline(\n",
    "        content,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "        temperature=None,  # Not needed when do_sample=False\n",
    "        top_p=None,        # Not needed when do_sample=False\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Ensure proper padding \n",
    "        max_length=10000\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        chat_response = seq['generated_text'][len(formatted_prompt):]\n",
    "            \n",
    "    return chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650a5cd1-0c21-446e-b5fe-97a7ffae764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Processing before_llamaoutput_clean.csv\n",
      "Here are the top 10 topics with their frequencies:\n",
      "\n",
      "Topic 1 : racism (83)\n",
      "Topic 2 : identity (36)\n",
      "Topic 3 : faith (21)\n",
      "Topic 4 : police brutality (10)\n",
      "Topic 5 : slavery (10)\n",
      "Topic 6 : politics (13)\n",
      "Topic 7 : education (14)\n",
      "Topic 8 : community (13)\n",
      "Topic 9 : relationship (13)\n",
      "Topic 10 : empowerment (14)\n",
      "After COVID\n",
      "Processing after_llamaoutput_clean.csv\n",
      "Here are the top 10 topics with their frequencies:\n",
      "\n",
      "Topic 1 : racism (102)\n",
      "Topic 2 : identity (44)\n",
      "Topic 3 : police brutality (8)\n",
      "Topic 4 : empowerment (22)\n",
      "Topic 5 : faith (17)\n",
      "Topic 6 : politics (14)\n",
      "Topic 7 : health (11)\n",
      "Topic 8 : community (16)\n",
      "Topic 9 : education (13)\n",
      "Topic 10 : justice (9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before COVID\")\n",
    "results = process_csv_file(\"before_llamaoutput_clean.csv\", 10)\n",
    "print(results)\n",
    "\n",
    "print(\"After COVID\")\n",
    "results = process_csv_file(\"after_llamaoutput_clean.csv\", 10)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
