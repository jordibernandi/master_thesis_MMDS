{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79678e7-897c-492c-b8c2-de0efc2738ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertopic accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import hdbscan\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efc4f23-d838-423e-aed1-59c0722533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5714, 21)\n",
      "(5726, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON files\n",
    "with open('transcripts_BEFORE_COVID.json', 'r') as file:\n",
    "    before_covid_data = json.load(file)\n",
    "\n",
    "with open('transcripts_AFTER_COVID.json', 'r') as file:\n",
    "    after_covid_data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "before_covid_df = pd.json_normalize(before_covid_data)\n",
    "after_covid_df = pd.json_normalize(after_covid_data)\n",
    "\n",
    "print(before_covid_df.shape)\n",
    "print(after_covid_df.shape)\n",
    "\n",
    "# Extract necessary columns\n",
    "texts_before = before_covid_df[['transcripts', 'channel.ideology']]\n",
    "texts_after = after_covid_df[['transcripts', 'channel.ideology']]\n",
    "\n",
    "# Get unique ideologies\n",
    "ideologies = texts_before['channel.ideology'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de31c673-7ce1-4e15-b426-c669e7919ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_before = texts_before[texts_before['channel.ideology'] == \"BLACK\"]\n",
    "texts_after = texts_after[texts_after['channel.ideology'] == \"BLACK\"]\n",
    "\n",
    "# List of ideologies\n",
    "ideologies = [\"BLACK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b297d2-b4d4-4c9f-8ae9-4c83514e0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# def clean_text(text):\n",
    "#      # Remove non-ascii characters\n",
    "#     text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "#     # Remove newlines and extra spaces\n",
    "#     text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "#     # Tokenize the text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "\n",
    "#     # Define unnecessary punctuation to remove\n",
    "#     unnecessary_punctuation = r'[“”\\'`~]'\n",
    "    \n",
    "#     # Remove unnecessary punctuation and special characters\n",
    "#     cleaned_sentences = [re.sub(unnecessary_punctuation, '', sentence) for sentence in sentences]\n",
    "\n",
    "#     # Optionally convert text to lowercase (comment this line if you want to keep the original case)\n",
    "#     # cleaned_sentences = [sentence.lower() for sentence in cleaned_sentences]\n",
    "\n",
    "#     # Join the cleaned sentences back into a single string\n",
    "#     cleaned_text = ' '.join(cleaned_sentences)\n",
    "\n",
    "#     return cleaned_text\n",
    "\n",
    "# # Assuming texts_before and texts_after are pandas DataFrames\n",
    "# texts_before['transcripts'] = texts_before['transcripts'].apply(clean_text)\n",
    "# texts_after['transcripts'] = texts_after['transcripts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e65ea14-f679-4724-afc9-e097a3559ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token size for texts_before: 1714.442731277533\n",
      "Average token size for texts_after: 1615.2521551724137\n",
      "Max token size for texts_before: 5207\n",
      "Max token size for texts_after: 4780\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to calculate average token size\n",
    "def get_average_token_size(texts, tokenizer):\n",
    "    total_tokens = 0\n",
    "    total_texts = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    average_size = total_tokens / total_texts if total_texts > 0 else 0\n",
    "    return average_size\n",
    "\n",
    "# Calculate average token size for texts_before and texts_after\n",
    "average_token_size_before = get_average_token_size(texts_before['transcripts'], tokenizer)\n",
    "average_token_size_after = get_average_token_size(texts_after['transcripts'], tokenizer)\n",
    "\n",
    "print(f'Average token size for texts_before: {average_token_size_before}')\n",
    "print(f'Average token size for texts_after: {average_token_size_after}')\n",
    "\n",
    "# Function to calculate max token size\n",
    "def get_max_token_size(texts, tokenizer):\n",
    "    max_size = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        max_size = max(max_size, len(tokens))\n",
    "    return max_size\n",
    "\n",
    "# Calculate max token size for texts_before and texts_after\n",
    "max_token_size_before = get_max_token_size(texts_before['transcripts'], tokenizer)\n",
    "max_token_size_after = get_max_token_size(texts_after['transcripts'], tokenizer)\n",
    "\n",
    "print(f'Max token size for texts_before: {max_token_size_before}')\n",
    "print(f'Max token size for texts_after: {max_token_size_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f499898-fe38-4f61-b14e-cf4af23ace36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_before['transcripts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b058656b-6f9d-459f-9a6a-543e25c6eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009625939fa840649aee87e7114f43b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23970ff1-9313-48b4-91c0-bc275c779110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d61c2a-b562-43d9-9b11-49cd6d0eb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab974a07-277d-43be-9aef-eaab1a7a762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b595451d29ed49bc9c082644001c166c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392b72e1a9e34ca7b164334cd221d04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Llama2 Model\n",
    "model_id_llama2 = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "tokenizer_llama2 = transformers.AutoTokenizer.from_pretrained(model_id_llama2)\n",
    "model_llama2 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_llama2,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_llama2.bfloat16()\n",
    "model_llama2.eval()\n",
    "\n",
    "# Initialize Llama3.1 Model\n",
    "model_id_llama3 = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer_llama3 = transformers.AutoTokenizer.from_pretrained(model_id_llama3)\n",
    "model_llama3 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_llama3,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_llama3.bfloat16()\n",
    "model_llama3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f183dec6-3927-47e1-8d61-c488e5d32743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generator pipeline for Llama2\n",
    "generator_llama2 = transformers.pipeline(\n",
    "    model=model_llama2, tokenizer=tokenizer_llama2,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Text generator pipeline for Llama3.1\n",
    "generator_llama3 = transformers.pipeline(\n",
    "    model=model_llama3, tokenizer=tokenizer_llama3,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a461c1-cad1-4efa-b94b-879e69ff1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt describes information given to all conversations\n",
    "system_prompt2 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt2 = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "\n",
    "[/INST] Environmental impacts of eating meat\n",
    "\"\"\"\n",
    "\n",
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt2 = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = system_prompt2 + example_prompt2 + main_prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb01d85-9505-4127-b70b-717af54a7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt3 = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful, respectful, and honest assistant for labeling topics. Your task is to generate a concise label for each topic provided. Please return only the label, consisting of one word or a short phrase, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "# Example User Prompt\n",
    "example_prompt3 = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "Environmental impacts of eating meat\n",
    "\"\"\"\n",
    "\n",
    "# Main Prompt for Labeling\n",
    "main_prompt3 = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = system_prompt3 + example_prompt3 + main_prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f4971c-685e-4bc0-bb48-a539194681e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyBERT\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# MMR\n",
    "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Text generation with Llama2\n",
    "llama2 = TextGeneration(generator_llama2, prompt=prompt2)\n",
    "\n",
    "# Text generation with Llama3.1\n",
    "llama3 = TextGeneration(generator_llama3, prompt=prompt3)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\": llama2,\n",
    "    \"Llama3\": llama3,\n",
    "    \"MMR\": mmr,\n",
    "}\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea825a3-225d-4bf2-9750-6177b4a3194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"\\n<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant for labeling topics.\\n<</SYS>>\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\\n\\n[/INST] Environmental impacts of eating meat\\n\\n[INST]\\nI have a topic that contains the following documents:\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\\n[/INST]\\nMeat consumption and environmental impact\"}]\n",
      "[{'generated_text': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful, and honest assistant for labeling topics. Your task is to generate a concise label for each topic provided. Please return only the label, consisting of one word or a short phrase, and nothing more.\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nEnvironmental impacts of eating meat\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nI have a topic that contains the following documents:\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMeat consumption\"}]\n"
     ]
    }
   ],
   "source": [
    "# Example text data for testing\n",
    "test_text = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\"\"\"\n",
    "\n",
    "test_prompt2 = system_prompt2 + example_prompt2 + main_prompt2.replace(\"[DOCUMENTS]\", test_text).replace(\"[KEYWORDS]\", \"meat, beef, eat, eating, emissions, steak, food, health, processed, chicken\")\n",
    "test_prompt3 = system_prompt3 + example_prompt3 + main_prompt3.replace(\"[DOCUMENTS]\", test_text).replace(\"[KEYWORDS]\", \"meat, beef, eat, eating, emissions, steak, food, health, processed, chicken\")\n",
    "\n",
    "try:\n",
    "    result_2 = generator_llama2(test_prompt2)\n",
    "    print(result_2)\n",
    "    result_3 = generator_llama3(test_prompt3)\n",
    "    print(result_3)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error during text generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86f46d7-b8b8-4292-8cd3-d109e83a5878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb9129e40b040ed9bdb5b2b8663adee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df36a8af874f4409a2728e715e561e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def precalculate_embeddings(texts, embedding_model):\n",
    "    embeddings_dict = {}\n",
    "    ideologies = texts['channel.ideology'].unique()\n",
    "    for ideology in ideologies:\n",
    "        ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "        embeddings = embedding_model.encode(ideology_texts, show_progress_bar=True)\n",
    "        embeddings_dict[ideology] = embeddings\n",
    "    return embeddings_dict\n",
    "\n",
    "# Pre-calculate embeddings for before and after COVID\n",
    "embeddings_before = precalculate_embeddings(texts_before, embedding_model)\n",
    "embeddings_after = precalculate_embeddings(texts_after, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6032fbf-67cc-4a58-81ad-9e5ae90487fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_embeddings(embeddings):\n",
    "    for ideology, emb in embeddings.items():\n",
    "        if np.any(np.isnan(emb)) or np.any(np.isinf(emb)):\n",
    "            print(f\"Invalid embeddings detected for ideology: {ideology}\")\n",
    "\n",
    "check_embeddings(embeddings_before)\n",
    "check_embeddings(embeddings_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0cb2b8-cadb-4312-99f6-e6b883062817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology, embeddings, vectorizer_model=None, ctfidf_model=None, representation_model=None):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, \n",
    "        vectorizer_model=vectorizer_model, \n",
    "        ctfidf_model=ctfidf_model, \n",
    "        representation_model=representation_model,\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts, embeddings[ideology])\n",
    "    return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ab7b33-ff51-4d67-b310-0bc0c516b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [02:58<00:00, 19.79s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:33<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [04:41<00:00, 25.57s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:35<00:00,  3.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# # List of ideologies\n",
    "# ideologies = texts_before['channel.ideology'].unique()\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology, embeddings_before, vectorizer_model=vectorizer_model, \n",
    "            ctfidf_model=ctfidf_model, representation_model=representation_model\n",
    "        )\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology, embeddings_after, vectorizer_model=vectorizer_model, \n",
    "            ctfidf_model=ctfidf_model, representation_model=representation_model\n",
    "        )\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ad0696-1aaf-47a0-9d2b-cbff51218fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>Llama2</th>\n",
       "      <th>Llama3</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>121</td>\n",
       "      <td>-1_like_don_black_going</td>\n",
       "      <td>[like, don, black, going, just, people, know, ...</td>\n",
       "      <td>[relationship, community, money, love, video, ...</td>\n",
       "      <td>[ sierp (\\n̂\\nuclide\\n̂\\n, , , , , , , , , ]</td>\n",
       "      <td>[Black Economic Empowerment, , , , , , , , , ]</td>\n",
       "      <td>[like, black, just, know, say, man, think, hai...</td>\n",
       "      <td>[Hey guys, so I wanted to come on here and tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0_going_high_people_lord</td>\n",
       "      <td>[going, high, people, lord, know, spirit, said...</td>\n",
       "      <td>[prophecy, prophecies, messiah, scripture, scr...</td>\n",
       "      <td>[ Unterscheidung\\\\ Hinweis everybody sierp̂, ,...</td>\n",
       "      <td>[Empire Destruction and Restoration, , , , , ,...</td>\n",
       "      <td>[high, lord, know, said, shall, time, book, co...</td>\n",
       "      <td>[Thy kingdom come, thy will be done, on earth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1_police_don_know_just</td>\n",
       "      <td>[police, don, know, just, going, people, got, ...</td>\n",
       "      <td>[police, cops, investigation, situation, video...</td>\n",
       "      <td>[ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Police accountability, , , , , , , , , ]</td>\n",
       "      <td>[police, people, like, black, look, young, vid...</td>\n",
       "      <td>[What's going on?  This is My Life in a Shot. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2_africa_yeah_thank_ghana</td>\n",
       "      <td>[africa, yeah, thank, ghana, like, know, come,...</td>\n",
       "      <td>[africa, blacksit, gambia, african, nigeria, n...</td>\n",
       "      <td>[Љ\\nЋЏ\\nЪ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\...</td>\n",
       "      <td>[African diaspora experiences, , , , , , , , , ]</td>\n",
       "      <td>[africa, thank, ghana, nigeria, african, say, ...</td>\n",
       "      <td>[yeah yeah we make home i make homemade ice cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>3_black_folk_black folk_white</td>\n",
       "      <td>[black, folk, black folk, white, rights, peopl...</td>\n",
       "      <td>[blacks, civil rights, slavery, slaves, black ...</td>\n",
       "      <td>[ kwiet paździer sierp Einzeln Bedeut̶̂ everyb...</td>\n",
       "      <td>[Racial disparities and systemic injustices, ,...</td>\n",
       "      <td>[black folk, shut shut shut, black people, say...</td>\n",
       "      <td>[Hey, what's up guys?  I'm Dr.  Boyce Watkins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>4_yeah_shit_man_got</td>\n",
       "      <td>[yeah, shit, man, got, movie, like, blackout, ...</td>\n",
       "      <td>[movie, monday monday monday, ha ha, doing shi...</td>\n",
       "      <td>[ЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Celebrity relationships and marriage expectat...</td>\n",
       "      <td>[man, movie, like, blackout, monday monday, pi...</td>\n",
       "      <td>[yo hello all right we're here.  what's going ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5_bethel_congregation_bethelites_circuit</td>\n",
       "      <td>[bethel, congregation, bethelites, circuit, el...</td>\n",
       "      <td>[bethelites, congregation, jehovah witnesses, ...</td>\n",
       "      <td>[ kwiet Einzelnἱ everybodyund, , , , , , , , , ]</td>\n",
       "      <td>[Life at Bethel, , , , , , , , , ]</td>\n",
       "      <td>[congregation, bethelites, elders, jehovah, so...</td>\n",
       "      <td>[It was really bad.  That was the whole focus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>6_right_bible_god_read</td>\n",
       "      <td>[right, bible, god, read, man, verse, people, ...</td>\n",
       "      <td>[according bible, prophet, deuteronomy, jewish...</td>\n",
       "      <td>[ЉЉЪЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Biblical Identity and Heritage, , , , , , , ,...</td>\n",
       "      <td>[bible, read, verse, know, okay, brother, shal...</td>\n",
       "      <td>[Look, there's the Black Holocaust.  I knew it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>7_women_know_men_like</td>\n",
       "      <td>[women, know, men, like, chick, pill, black, d...</td>\n",
       "      <td>[black women, white women, documentary, oprah,...</td>\n",
       "      <td>[ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Red Pill Culture, , , , , , , , , ]</td>\n",
       "      <td>[like, pill, woman, want, black men, porn, mic...</td>\n",
       "      <td>[The Isis graphic novel.  Indiegogo is live.  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                      Name  \\\n",
       "0     -1    121                   -1_like_don_black_going   \n",
       "1      0     61                  0_going_high_people_lord   \n",
       "2      1     55                    1_police_don_know_just   \n",
       "3      2     54                 2_africa_yeah_thank_ghana   \n",
       "4      3     38             3_black_folk_black folk_white   \n",
       "5      4     33                       4_yeah_shit_man_got   \n",
       "6      5     32  5_bethel_congregation_bethelites_circuit   \n",
       "7      6     31                    6_right_bible_god_read   \n",
       "8      7     29                     7_women_know_men_like   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [like, don, black, going, just, people, know, ...   \n",
       "1  [going, high, people, lord, know, spirit, said...   \n",
       "2  [police, don, know, just, going, people, got, ...   \n",
       "3  [africa, yeah, thank, ghana, like, know, come,...   \n",
       "4  [black, folk, black folk, white, rights, peopl...   \n",
       "5  [yeah, shit, man, got, movie, like, blackout, ...   \n",
       "6  [bethel, congregation, bethelites, circuit, el...   \n",
       "7  [right, bible, god, read, man, verse, people, ...   \n",
       "8  [women, know, men, like, chick, pill, black, d...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [relationship, community, money, love, video, ...   \n",
       "1  [prophecy, prophecies, messiah, scripture, scr...   \n",
       "2  [police, cops, investigation, situation, video...   \n",
       "3  [africa, blacksit, gambia, african, nigeria, n...   \n",
       "4  [blacks, civil rights, slavery, slaves, black ...   \n",
       "5  [movie, monday monday monday, ha ha, doing shi...   \n",
       "6  [bethelites, congregation, jehovah witnesses, ...   \n",
       "7  [according bible, prophet, deuteronomy, jewish...   \n",
       "8  [black women, white women, documentary, oprah,...   \n",
       "\n",
       "                                              Llama2  \\\n",
       "0       [ sierp (\\n̂\\nuclide\\n̂\\n, , , , , , , , , ]   \n",
       "1  [ Unterscheidung\\\\ Hinweis everybody sierp̂, ,...   \n",
       "2  [ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "3  [Љ\\nЋЏ\\nЪ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\nЉ\\...   \n",
       "4  [ kwiet paździer sierp Einzeln Bedeut̶̂ everyb...   \n",
       "5  [ЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "6   [ kwiet Einzelnἱ everybodyund, , , , , , , , , ]   \n",
       "7  [ЉЉЪЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "8  [ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "\n",
       "                                              Llama3  \\\n",
       "0     [Black Economic Empowerment, , , , , , , , , ]   \n",
       "1  [Empire Destruction and Restoration, , , , , ,...   \n",
       "2          [Police accountability, , , , , , , , , ]   \n",
       "3   [African diaspora experiences, , , , , , , , , ]   \n",
       "4  [Racial disparities and systemic injustices, ,...   \n",
       "5  [Celebrity relationships and marriage expectat...   \n",
       "6                 [Life at Bethel, , , , , , , , , ]   \n",
       "7  [Biblical Identity and Heritage, , , , , , , ,...   \n",
       "8               [Red Pill Culture, , , , , , , , , ]   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [like, black, just, know, say, man, think, hai...   \n",
       "1  [high, lord, know, said, shall, time, book, co...   \n",
       "2  [police, people, like, black, look, young, vid...   \n",
       "3  [africa, thank, ghana, nigeria, african, say, ...   \n",
       "4  [black folk, shut shut shut, black people, say...   \n",
       "5  [man, movie, like, blackout, monday monday, pi...   \n",
       "6  [congregation, bethelites, elders, jehovah, so...   \n",
       "7  [bible, read, verse, know, okay, brother, shal...   \n",
       "8  [like, pill, woman, want, black men, porn, mic...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Hey guys, so I wanted to come on here and tal...  \n",
       "1  [Thy kingdom come, thy will be done, on earth ...  \n",
       "2  [What's going on?  This is My Life in a Shot. ...  \n",
       "3  [yeah yeah we make home i make homemade ice cr...  \n",
       "4  [Hey, what's up guys?  I'm Dr.  Boyce Watkins ...  \n",
       "5  [yo hello all right we're here.  what's going ...  \n",
       "6  [It was really bad.  That was the whole focus ...  \n",
       "7  [Look, there's the Black Holocaust.  I knew it...  \n",
       "8  [The Isis graphic novel.  Indiegogo is live.  ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>Llama2</th>\n",
       "      <th>Llama3</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>138</td>\n",
       "      <td>-1_know_people_just_don</td>\n",
       "      <td>[know, people, just, don, going, like, right, ...</td>\n",
       "      <td>[witness, witnesses, video, watchtower, jehova...</td>\n",
       "      <td>[ЉЉЋЉЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЋЉЉЉЉЉЉЉЋЉЉЉЉ...</td>\n",
       "      <td>[Jehovah's Witnesses criticism, , , , , , , , , ]</td>\n",
       "      <td>[people, just, jehovah, family, say, black, me...</td>\n",
       "      <td>[Shalom, shalom, shalom.  Just want to get on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0_police_know_like_got</td>\n",
       "      <td>[police, know, like, got, going, shit, just, y...</td>\n",
       "      <td>[officer, police, cops, officers, cop, crime, ...</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[Police brutality and missing persons investig...</td>\n",
       "      <td>[police, like, car, niggas, black, want, bro, ...</td>\n",
       "      <td>[I'm like, yo, you damaged my vehicle.  Let me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1_right_read_israel_people</td>\n",
       "      <td>[right, read, israel, people, going, know, hig...</td>\n",
       "      <td>[israelites, judah, prophecy, moses, scripture...</td>\n",
       "      <td>[ЉЪЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Identity of the Chosen People, , , , , , , , , ]</td>\n",
       "      <td>[read, israel, know, verse, bible, said, come,...</td>\n",
       "      <td>[Good rising brethren, this is Big Judah comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2_women_men_black_woman</td>\n",
       "      <td>[women, men, black, woman, black women, white,...</td>\n",
       "      <td>[black women, white women, white woman, black ...</td>\n",
       "      <td>[ЉЉЋЪЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Dating dynamics between black and white women...</td>\n",
       "      <td>[black, black men, female, know, white women, ...</td>\n",
       "      <td>[I was telling Warren Lance that I like a good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3_africa_african_france_countries</td>\n",
       "      <td>[africa, african, france, countries, continent...</td>\n",
       "      <td>[african leaders, apartheid, africa, africans,...</td>\n",
       "      <td>[ЉЋЪЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЋЉЉЉ...</td>\n",
       "      <td>[African sovereignty and self-definition, , , ...</td>\n",
       "      <td>[africa, france, countries, continent, french,...</td>\n",
       "      <td>[Permit me to annoy you.  In many African nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4_covid_thank_know_black folk</td>\n",
       "      <td>[covid, thank, know, black folk, masks, folk, ...</td>\n",
       "      <td>[coronavirus, covid 19, quarantine, pandemic, ...</td>\n",
       "      <td>[ЉЉЉЉ\\nЉЉЉЉЪЉЉЉЉЉЉЉЉЋЉЏЉЉЉ\\nЉЉЉЉ\\nЉ\\nЉЉ\\nЉЉЉЉЉ...</td>\n",
       "      <td>[African innovation during COVID-19, , , , , ,...</td>\n",
       "      <td>[covid, black folk, masks, vaccine, coronaviru...</td>\n",
       "      <td>[What you just heard was an interview by phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5_home home_home home home_purchase_purchase t...</td>\n",
       "      <td>[home home, home home home, purchase, purchase...</td>\n",
       "      <td>[kitchen, farmer, black farmers, building, far...</td>\n",
       "      <td>[ЉЋЉЪЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[African expatriate experiences, , , , , , , ,...</td>\n",
       "      <td>[home home, today purchase tracks, donkey, lan...</td>\n",
       "      <td>[You don't need any more of those. Yes, depend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>6_black_black people_biden_white</td>\n",
       "      <td>[black, black people, biden, white, people, jo...</td>\n",
       "      <td>[black leaders, care black lives, black commun...</td>\n",
       "      <td>[ЉЉЏЋЉЉЉЉЉЉЉЉЉЉ\\nЉЉ\\nЉЉЉ\\nЉЉЉЉ\\nЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Black empowerment vs systemic oppression, , ,...</td>\n",
       "      <td>[black people, biden, white, black leaders, co...</td>\n",
       "      <td>[Lie can go around the world three times befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>7_oh lord_lord_lord oh_oh lord oh</td>\n",
       "      <td>[oh lord, lord, lord oh, oh lord oh, lord oh l...</td>\n",
       "      <td>[god, hallelujah, lord, jesus, pray, peace, lo...</td>\n",
       "      <td>[ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЋЉЉ...</td>\n",
       "      <td>[Spiritual warfare and activism, , , , , , , ,...</td>\n",
       "      <td>[lord oh lord, god, water children, enemy, lea...</td>\n",
       "      <td>[Them peace, they ain't no night.  We not gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>8_mean_man_yo_don</td>\n",
       "      <td>[mean, man, yo, don, ain, shit, people, day, e...</td>\n",
       "      <td>[disrespect, nigga, man don, situation, youtub...</td>\n",
       "      <td>[ЉЋЉЪЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Black identity politics, , , , , , , , , ]</td>\n",
       "      <td>[mean, yo, ain, end day, black, videos, black ...</td>\n",
       "      <td>[So there have been some developments since we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9_brothers sisters_sisters_vision_brothers</td>\n",
       "      <td>[brothers sisters, sisters, vision, brothers, ...</td>\n",
       "      <td>[holy spirit, amen amen blessings, visions, am...</td>\n",
       "      <td>[ЉЉЉЉ\\nЉЏЉЉЉЉЉЉ\\nЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Three Days of Darkness Prophecy, , , , , , , ...</td>\n",
       "      <td>[brothers, spirit, clay, kanye, know, days dar...</td>\n",
       "      <td>[Hi guys, this is your sister Karen Gidden in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1    138                            -1_know_people_just_don   \n",
       "1       0     83                             0_police_know_like_got   \n",
       "2       1     68                         1_right_read_israel_people   \n",
       "3       2     41                            2_women_men_black_woman   \n",
       "4       3     26                  3_africa_african_france_countries   \n",
       "5       4     22                      4_covid_thank_know_black folk   \n",
       "6       5     21  5_home home_home home home_purchase_purchase t...   \n",
       "7       6     20                   6_black_black people_biden_white   \n",
       "8       7     19                  7_oh lord_lord_lord oh_oh lord oh   \n",
       "9       8     14                                  8_mean_man_yo_don   \n",
       "10      9     12         9_brothers sisters_sisters_vision_brothers   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [know, people, just, don, going, like, right, ...   \n",
       "1   [police, know, like, got, going, shit, just, y...   \n",
       "2   [right, read, israel, people, going, know, hig...   \n",
       "3   [women, men, black, woman, black women, white,...   \n",
       "4   [africa, african, france, countries, continent...   \n",
       "5   [covid, thank, know, black folk, masks, folk, ...   \n",
       "6   [home home, home home home, purchase, purchase...   \n",
       "7   [black, black people, biden, white, people, jo...   \n",
       "8   [oh lord, lord, lord oh, oh lord oh, lord oh l...   \n",
       "9   [mean, man, yo, don, ain, shit, people, day, e...   \n",
       "10  [brothers sisters, sisters, vision, brothers, ...   \n",
       "\n",
       "                                              KeyBERT  \\\n",
       "0   [witness, witnesses, video, watchtower, jehova...   \n",
       "1   [officer, police, cops, officers, cop, crime, ...   \n",
       "2   [israelites, judah, prophecy, moses, scripture...   \n",
       "3   [black women, white women, white woman, black ...   \n",
       "4   [african leaders, apartheid, africa, africans,...   \n",
       "5   [coronavirus, covid 19, quarantine, pandemic, ...   \n",
       "6   [kitchen, farmer, black farmers, building, far...   \n",
       "7   [black leaders, care black lives, black commun...   \n",
       "8   [god, hallelujah, lord, jesus, pray, peace, lo...   \n",
       "9   [disrespect, nigga, man don, situation, youtub...   \n",
       "10  [holy spirit, amen amen blessings, visions, am...   \n",
       "\n",
       "                                               Llama2  \\\n",
       "0   [ЉЉЋЉЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЋЉЉЉЉЉЉЉЋЉЉЉЉ...   \n",
       "1                                [, , , , , , , , , ]   \n",
       "2   [ЉЪЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "3   [ЉЉЋЪЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "4   [ЉЋЪЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЋЉЉЉ...   \n",
       "5   [ЉЉЉЉ\\nЉЉЉЉЪЉЉЉЉЉЉЉЉЋЉЏЉЉЉ\\nЉЉЉЉ\\nЉ\\nЉЉ\\nЉЉЉЉЉ...   \n",
       "6   [ЉЋЉЪЉЉЏЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "7   [ЉЉЏЋЉЉЉЉЉЉЉЉЉЉ\\nЉЉ\\nЉЉЉ\\nЉЉЉЉ\\nЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "8   [ЉЋЪЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЋЉЉ...   \n",
       "9   [ЉЋЉЪЉЉЉЉЉЉЉЉЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "10  [ЉЉЉЉ\\nЉЏЉЉЉЉЉЉ\\nЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "\n",
       "                                               Llama3  \\\n",
       "0   [Jehovah's Witnesses criticism, , , , , , , , , ]   \n",
       "1   [Police brutality and missing persons investig...   \n",
       "2   [Identity of the Chosen People, , , , , , , , , ]   \n",
       "3   [Dating dynamics between black and white women...   \n",
       "4   [African sovereignty and self-definition, , , ...   \n",
       "5   [African innovation during COVID-19, , , , , ,...   \n",
       "6   [African expatriate experiences, , , , , , , ,...   \n",
       "7   [Black empowerment vs systemic oppression, , ,...   \n",
       "8   [Spiritual warfare and activism, , , , , , , ,...   \n",
       "9         [Black identity politics, , , , , , , , , ]   \n",
       "10  [Three Days of Darkness Prophecy, , , , , , , ...   \n",
       "\n",
       "                                                  MMR  \\\n",
       "0   [people, just, jehovah, family, say, black, me...   \n",
       "1   [police, like, car, niggas, black, want, bro, ...   \n",
       "2   [read, israel, know, verse, bible, said, come,...   \n",
       "3   [black, black men, female, know, white women, ...   \n",
       "4   [africa, france, countries, continent, french,...   \n",
       "5   [covid, black folk, masks, vaccine, coronaviru...   \n",
       "6   [home home, today purchase tracks, donkey, lan...   \n",
       "7   [black people, biden, white, black leaders, co...   \n",
       "8   [lord oh lord, god, water children, enemy, lea...   \n",
       "9   [mean, yo, ain, end day, black, videos, black ...   \n",
       "10  [brothers, spirit, clay, kanye, know, days dar...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [Shalom, shalom, shalom.  Just want to get on ...  \n",
       "1   [I'm like, yo, you damaged my vehicle.  Let me...  \n",
       "2   [Good rising brethren, this is Big Judah comin...  \n",
       "3   [I was telling Warren Lance that I like a good...  \n",
       "4   [Permit me to annoy you.  In many African nati...  \n",
       "5   [What you just heard was an interview by phone...  \n",
       "6   [You don't need any more of those. Yes, depend...  \n",
       "7   [Lie can go around the world three times befor...  \n",
       "8   [Them peace, they ain't no night.  We not gonn...  \n",
       "9   [So there have been some developments since we...  \n",
       "10  [Hi guys, this is your sister Karen Gidden in ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "\n",
    "    model_before\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "    # display(model_before.visualize_hierarchy())\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n",
    "    # display(model_after.visualize_hierarchy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
