{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79678e7-897c-492c-b8c2-de0efc2738ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bertopic accelerate bitsandbytes xformers adjustText\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import hdbscan\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efc4f23-d838-423e-aed1-59c0722533a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5714, 21)\n",
      "(5726, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON files\n",
    "with open('transcripts_BEFORE_COVID.json', 'r') as file:\n",
    "    before_covid_data = json.load(file)\n",
    "\n",
    "with open('transcripts_AFTER_COVID.json', 'r') as file:\n",
    "    after_covid_data = json.load(file)\n",
    "\n",
    "# Convert to DataFrame\n",
    "before_covid_df = pd.json_normalize(before_covid_data)\n",
    "after_covid_df = pd.json_normalize(after_covid_data)\n",
    "\n",
    "print(before_covid_df.shape)\n",
    "print(after_covid_df.shape)\n",
    "\n",
    "# Extract necessary columns\n",
    "texts_before = before_covid_df[['transcripts', 'channel.ideology']]\n",
    "texts_after = after_covid_df[['transcripts', 'channel.ideology']]\n",
    "\n",
    "# Get unique ideologies\n",
    "ideologies = texts_before['channel.ideology'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0fb213-c91d-46ee-b1f6-befa869c4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_before = texts_before[texts_before['channel.ideology'] == \"BLACK\"]\n",
    "texts_after = texts_after[texts_after['channel.ideology'] == \"BLACK\"]\n",
    "\n",
    "# List of ideologies\n",
    "ideologies = [\"BLACK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b297d2-b4d4-4c9f-8ae9-4c83514e0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# def clean_text(text):\n",
    "#      # Remove non-ascii characters\n",
    "#     text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "#     # Remove newlines and extra spaces\n",
    "#     text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "#     # Tokenize the text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "\n",
    "#     # Define unnecessary punctuation to remove\n",
    "#     unnecessary_punctuation = r'[“”\\'`~]'\n",
    "    \n",
    "#     # Remove unnecessary punctuation and special characters\n",
    "#     cleaned_sentences = [re.sub(unnecessary_punctuation, '', sentence) for sentence in sentences]\n",
    "\n",
    "#     # Optionally convert text to lowercase (comment this line if you want to keep the original case)\n",
    "#     # cleaned_sentences = [sentence.lower() for sentence in cleaned_sentences]\n",
    "\n",
    "#     # Join the cleaned sentences back into a single string\n",
    "#     cleaned_text = ' '.join(cleaned_sentences)\n",
    "\n",
    "#     return cleaned_text\n",
    "\n",
    "# # Assuming texts_before and texts_after are pandas DataFrames\n",
    "# texts_before['transcripts'] = texts_before['transcripts'].apply(clean_text)\n",
    "# texts_after['transcripts'] = texts_after['transcripts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e65ea14-f679-4724-afc9-e097a3559ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token size for texts_before: 1714.442731277533\n",
      "Average token size for texts_after: 1615.2521551724137\n",
      "Max token size for texts_before: 5207\n",
      "Max token size for texts_after: 4780\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to calculate average token size\n",
    "def get_average_token_size(texts, tokenizer):\n",
    "    total_tokens = 0\n",
    "    total_texts = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    average_size = total_tokens / total_texts if total_texts > 0 else 0\n",
    "    return average_size\n",
    "\n",
    "# Calculate average token size for texts_before and texts_after\n",
    "average_token_size_before = get_average_token_size(texts_before['transcripts'], tokenizer)\n",
    "average_token_size_after = get_average_token_size(texts_after['transcripts'], tokenizer)\n",
    "\n",
    "print(f'Average token size for texts_before: {average_token_size_before}')\n",
    "print(f'Average token size for texts_after: {average_token_size_after}')\n",
    "\n",
    "# Function to calculate max token size\n",
    "def get_max_token_size(texts, tokenizer):\n",
    "    max_size = 0\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        max_size = max(max_size, len(tokens))\n",
    "    return max_size\n",
    "\n",
    "# Calculate max token size for texts_before and texts_after\n",
    "max_token_size_before = get_max_token_size(texts_before['transcripts'], tokenizer)\n",
    "max_token_size_after = get_max_token_size(texts_after['transcripts'], tokenizer)\n",
    "\n",
    "print(f'Max token size for texts_before: {max_token_size_before}')\n",
    "print(f'Max token size for texts_after: {max_token_size_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f499898-fe38-4f61-b14e-cf4af23ace36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_before['transcripts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b058656b-6f9d-459f-9a6a-543e25c6eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df93cdaa26754ca6b2717da465ce5563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23970ff1-9313-48b4-91c0-bc275c779110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d61c2a-b562-43d9-9b11-49cd6d0eb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab974a07-277d-43be-9aef-eaab1a7a762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0a2cd2aea8419198dbb5404e1cf1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec82bb61434c44a7a363da4ff489470c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Llama2 Model\n",
    "model_id_llama2 = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "tokenizer_llama2 = transformers.AutoTokenizer.from_pretrained(model_id_llama2)\n",
    "model_llama2 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_llama2,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_llama2.bfloat16()\n",
    "model_llama2.eval()\n",
    "\n",
    "# Initialize Llama3.1 Model\n",
    "model_id_llama3 = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "tokenizer_llama3 = transformers.AutoTokenizer.from_pretrained(model_id_llama3)\n",
    "model_llama3 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_llama3,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model_llama3.bfloat16()\n",
    "model_llama3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f183dec6-3927-47e1-8d61-c488e5d32743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generator pipeline for Llama2\n",
    "generator_llama2 = transformers.pipeline(\n",
    "    model=model_llama2, tokenizer=tokenizer_llama2,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# Text generator pipeline for Llama3.1\n",
    "generator_llama3 = transformers.pipeline(\n",
    "    model=model_llama3, tokenizer=tokenizer_llama3,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a461c1-cad1-4efa-b94b-879e69ff1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt describes information given to all conversations\n",
    "system_prompt2 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt2 = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "\n",
    "[/INST] Environmental impacts of eating meat\n",
    "\"\"\"\n",
    "\n",
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt2 = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = system_prompt2 + example_prompt2 + main_prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34ea5ed-775f-4776-b87b-70992b700ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt3 = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful, respectful, and honest assistant for labeling topics. Your task is to generate a concise label for each topic provided. Please return only the label, consisting of one word or a short phrase, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "# Example User Prompt\n",
    "example_prompt3 = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "Environmental impacts of eating meat\n",
    "\"\"\"\n",
    "\n",
    "# Main Prompt for Labeling\n",
    "main_prompt3 = \"\"\"\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = system_prompt3 + example_prompt3 + main_prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f4971c-685e-4bc0-bb48-a539194681e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dac498ad694244aaeccc7ea167abc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a066901af845c08f143bd5f1b617ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d510bc6fddf4cf0b6808aac91a548d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484791b7c03450091087378c0ee3a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0383a3996f00404ca84efe59bbbb10c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37515e30cf3349a3bda08e0e80bada5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245dd06cda954e8da70822ff107db833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/iharsawi/.cache/torch/sentence_transformers/allenai_longformer-base-4096. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 8, Truncated Length: 8\n",
      "Original Sequence: This is a short sentence.\n",
      "Truncated Sequence: This is a short sentence.\n",
      "Truncated correctly: True\n",
      "\n",
      "Original Length: 6253, Truncated Length: 4096\n",
      "Original Sequence: This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. \n",
      "Truncated Sequence: This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length\n",
      "Truncated correctly: True\n",
      "\n",
      "Original Length: 6, Truncated Length: 6\n",
      "Original Sequence: Another short one.\n",
      "Truncated Sequence: Another short one.\n",
      "Truncated correctly: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KeyBERT\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# MMR\n",
    "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Text generation with Llama2\n",
    "llama2 = TextGeneration(generator_llama2, prompt=prompt2)\n",
    "\n",
    "# Text generation with Llama3.1\n",
    "llama3 = TextGeneration(generator_llama3, prompt=prompt3)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\": llama2,\n",
    "    \"Llama3\": llama3,\n",
    "    \"MMR\": mmr,\n",
    "}\n",
    "\n",
    "embedding_model = SentenceTransformer(\"allenai/longformer-base-4096\")\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "def truncate_sequences(sequences, max_length):\n",
    "    truncated_sequences = []\n",
    "    for seq in sequences:\n",
    "        tokenized = tokenizer(seq, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        truncated_sequences.append(tokenizer.decode(tokenized.input_ids[0], skip_special_tokens=True))\n",
    "    return truncated_sequences\n",
    "\n",
    "def verify_truncation_logic(sequences, max_length):\n",
    "    for seq in sequences:\n",
    "        tokenized = tokenizer(seq, return_tensors='pt')\n",
    "        original_length = len(tokenized.input_ids[0])\n",
    "        \n",
    "        truncated_seq = truncate_sequences([seq], max_length)[0]\n",
    "        truncated_tokenized = tokenizer(truncated_seq, return_tensors='pt')\n",
    "        truncated_length = len(truncated_tokenized.input_ids[0])\n",
    "        \n",
    "        print(f\"Original Length: {original_length}, Truncated Length: {truncated_length}\")\n",
    "        print(f\"Original Sequence: {seq}\")\n",
    "        print(f\"Truncated Sequence: {truncated_seq}\")\n",
    "        print(f\"Truncated correctly: {truncated_length <= max_length}\\n\")\n",
    "\n",
    "# Example sequences\n",
    "sequences = [\n",
    "    \"This is a short sentence.\",\n",
    "    \"This is a longer sentence that will be tokenized and truncated if it exceeds the maximum length set for the tokenizer. \" * 250, # artificially long sequence\n",
    "    \"Another short one.\"\n",
    "]\n",
    "\n",
    "max_length = 4096\n",
    "verify_truncation_logic(sequences, max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea825a3-225d-4bf2-9750-6177b4a3194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"\\n<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant for labeling topics.\\n<</SYS>>\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\\n\\n[/INST] Environmental impacts of eating meat\\n\\n[INST]\\nI have a topic that contains the following documents:\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\\n[/INST]\\nMeat consumption and environmental impact\"}]\n",
      "[{'generated_text': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a helpful, respectful, and honest assistant for labeling topics. Your task is to generate a concise label for each topic provided. Please return only the label, consisting of one word or a short phrase, and nothing more.\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nEnvironmental impacts of eating meat\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\nI have a topic that contains the following documents:\\n\\nI have a topic that contains the following documents:\\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\\n- Meat, but especially beef, is the word food in terms of emissions.\\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\n\\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\\n\\nBased on the information about the topic above, please create a short label of this topic. Make sure to only return the label and nothing more.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\nMeat consumption\"}]\n"
     ]
    }
   ],
   "source": [
    "# Example text data for testing\n",
    "test_text = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\"\"\"\n",
    "\n",
    "test_prompt2 = system_prompt2 + example_prompt2 + main_prompt2.replace(\"[DOCUMENTS]\", test_text).replace(\"[KEYWORDS]\", \"meat, beef, eat, eating, emissions, steak, food, health, processed, chicken\")\n",
    "test_prompt3 = system_prompt3 + example_prompt3 + main_prompt3.replace(\"[DOCUMENTS]\", test_text).replace(\"[KEYWORDS]\", \"meat, beef, eat, eating, emissions, steak, food, health, processed, chicken\")\n",
    "\n",
    "try:\n",
    "    result_2 = generator_llama2(test_prompt2)\n",
    "    print(result_2)\n",
    "    result_3 = generator_llama3(test_prompt3)\n",
    "    print(result_3)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error during text generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c86f46d7-b8b8-4292-8cd3-d109e83a5878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d4a9cfb3b0406993760d6b97a02103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db95c5f38c2b42d1a103a3d44d2a27cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def precalculate_embeddings(texts, embedding_model):\n",
    "    embeddings_dict = {}\n",
    "    ideologies = texts['channel.ideology'].unique()\n",
    "    for ideology in ideologies:\n",
    "        ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "        ideology_texts = truncate_sequences(ideology_texts, 4096)\n",
    "        embeddings = embedding_model.encode(ideology_texts, show_progress_bar=True)\n",
    "        embeddings_dict[ideology] = embeddings\n",
    "    return embeddings_dict\n",
    "\n",
    "# Pre-calculate embeddings for before and after COVID\n",
    "embeddings_before = precalculate_embeddings(texts_before, embedding_model)\n",
    "embeddings_after = precalculate_embeddings(texts_after, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6032fbf-67cc-4a58-81ad-9e5ae90487fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_embeddings(embeddings):\n",
    "    for ideology, emb in embeddings.items():\n",
    "        if np.any(np.isnan(emb)) or np.any(np.isinf(emb)):\n",
    "            print(f\"Invalid embeddings detected for ideology: {ideology}\")\n",
    "\n",
    "check_embeddings(embeddings_before)\n",
    "check_embeddings(embeddings_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0cb2b8-cadb-4312-99f6-e6b883062817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train BERTopic with specific configurations\n",
    "def train_bertopic_for_ideology(texts, ideology, embeddings, vectorizer_model=None, ctfidf_model=None, representation_model=None):\n",
    "    ideology_texts = texts[texts['channel.ideology'] == ideology]['transcripts'].tolist()\n",
    "    ideology_texts = truncate_sequences(ideology_texts, 4096)\n",
    "    print(f\"Training model for ideology: {ideology} with {len(ideology_texts)} texts.\")\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, \n",
    "        vectorizer_model=vectorizer_model, \n",
    "        ctfidf_model=ctfidf_model, \n",
    "        representation_model=representation_model,\n",
    "        # Hyperparameters\n",
    "        top_n_words=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(ideology_texts, embeddings[ideology])\n",
    "    return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ab7b33-ff51-4d67-b310-0bc0c516b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before COVID\n",
      "Training model for ideology: BLACK with 454 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.12s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After COVID\n",
      "Training model for ideology: BLACK with 464 texts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:35<00:00, 17.57s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# # List of ideologies\n",
    "# ideologies = texts_before['channel.ideology'].unique()\n",
    "\n",
    "# Dictionary to store models and topics\n",
    "models_before = {}\n",
    "models_after = {}\n",
    "\n",
    "for ideology in ideologies:\n",
    "    try:\n",
    "        print(\"Before COVID\")\n",
    "        model_before, topics_before, probs_before = train_bertopic_for_ideology(\n",
    "            texts_before, ideology, embeddings_before, vectorizer_model=vectorizer_model, \n",
    "            ctfidf_model=ctfidf_model, representation_model=representation_model\n",
    "        )\n",
    "        models_before[ideology] = (model_before, topics_before, probs_before)\n",
    "        \n",
    "        print(\"After COVID\")\n",
    "        model_after, topics_after, probs_after = train_bertopic_for_ideology(\n",
    "            texts_after, ideology, embeddings_after, vectorizer_model=vectorizer_model, \n",
    "            ctfidf_model=ctfidf_model, representation_model=representation_model\n",
    "        )\n",
    "        models_after[ideology] = (model_after, topics_after, probs_after)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for ideology {ideology}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ad0696-1aaf-47a0-9d2b-cbff51218fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations Before COVID for BLACK:\n",
      "\n",
      "Number of Topic: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>Llama2</th>\n",
       "      <th>Llama3</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>0_know_people_going_like</td>\n",
       "      <td>[know, people, going, like, don, just, right, ...</td>\n",
       "      <td>[black people, everybody, believe, trying, say...</td>\n",
       "      <td>[, , , , , , , , , ]</td>\n",
       "      <td>[Black American social issues, , , , , , , , , ]</td>\n",
       "      <td>[know, people, just, got, come, think, okay, r...</td>\n",
       "      <td>[This is your brother Malcolm coming at you wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1_jesus jesus_thank_conference_jesus jesus jesus</td>\n",
       "      <td>[jesus jesus, thank, conference, jesus jesus j...</td>\n",
       "      <td>[miami international airport, embassy suites h...</td>\n",
       "      <td>[Stephen Darby Ministries Conferences, , , , ,...</td>\n",
       "      <td>[Christian ministry events, , , , , , , , , ]</td>\n",
       "      <td>[jesus jesus, jesus jesus jesus, jesus, krispy...</td>\n",
       "      <td>[Hello, family.  Thank you for your continued ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                              Name  \\\n",
       "0      0    426                          0_know_people_going_like   \n",
       "1      1     28  1_jesus jesus_thank_conference_jesus jesus jesus   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, people, going, like, don, just, right, ...   \n",
       "1  [jesus jesus, thank, conference, jesus jesus j...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [black people, everybody, believe, trying, say...   \n",
       "1  [miami international airport, embassy suites h...   \n",
       "\n",
       "                                              Llama2  \\\n",
       "0                               [, , , , , , , , , ]   \n",
       "1  [Stephen Darby Ministries Conferences, , , , ,...   \n",
       "\n",
       "                                             Llama3  \\\n",
       "0  [Black American social issues, , , , , , , , , ]   \n",
       "1     [Christian ministry events, , , , , , , , , ]   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [know, people, just, got, come, think, okay, r...   \n",
       "1  [jesus jesus, jesus jesus jesus, jesus, krispy...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [This is your brother Malcolm coming at you wi...  \n",
       "1  [Hello, family.  Thank you for your continued ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Representations After COVID for BLACK:\n",
      "\n",
      "Number of Topic: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>Llama2</th>\n",
       "      <th>Llama3</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>0_know_people_going_like</td>\n",
       "      <td>[know, people, going, like, don, just, black, ...</td>\n",
       "      <td>[black people, everybody, don want, believe, t...</td>\n",
       "      <td>[ЉЪЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...</td>\n",
       "      <td>[Racism and Identity Deception, , , , , , , , , ]</td>\n",
       "      <td>[know, people, just, come, think, okay, oh, ye...</td>\n",
       "      <td>[Good rising, brethren.  This is Big Judah, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>1_thank_chardian_pearson_light</td>\n",
       "      <td>[thank, chardian, pearson, light, carlton, car...</td>\n",
       "      <td>[informative educational videos, suggestions n...</td>\n",
       "      <td>[Carlton Pearson and Energy Healing, , , , , ,...</td>\n",
       "      <td>[African culture and spirituality, , , , , , ,...</td>\n",
       "      <td>[carlton, carlton pearson, triggers, steal awa...</td>\n",
       "      <td>[Hello, I'm Carlton Pearson.  Listen to this w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                            Name  \\\n",
       "0      0    417        0_know_people_going_like   \n",
       "1      1     47  1_thank_chardian_pearson_light   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [know, people, going, like, don, just, black, ...   \n",
       "1  [thank, chardian, pearson, light, carlton, car...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [black people, everybody, don want, believe, t...   \n",
       "1  [informative educational videos, suggestions n...   \n",
       "\n",
       "                                              Llama2  \\\n",
       "0  [ЉЪЋЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉЉ...   \n",
       "1  [Carlton Pearson and Energy Healing, , , , , ,...   \n",
       "\n",
       "                                              Llama3  \\\n",
       "0  [Racism and Identity Deception, , , , , , , , , ]   \n",
       "1  [African culture and spirituality, , , , , , ,...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [know, people, just, come, think, okay, oh, ye...   \n",
       "1  [carlton, carlton pearson, triggers, steal awa...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Good rising, brethren.  This is Big Judah, co...  \n",
       "1  [Hello, I'm Carlton Pearson.  Listen to this w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze and compare topics\n",
    "for ideology in ideologies:\n",
    "    model_before, topics_before, _ = models_before[ideology]\n",
    "    model_after, topics_after, _ = models_after[ideology]\n",
    "\n",
    "    model_before\n",
    "    \n",
    "    # Get topic representations\n",
    "    topics_info_before = model_before.get_topic_info()\n",
    "    topics_info_after = model_after.get_topic_info()\n",
    "    \n",
    "    print(f\"\\nTopic Representations Before COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_before)}\")\n",
    "    display(topics_info_before)\n",
    "    # display(model_before.visualize_hierarchy())\n",
    "\n",
    "    print(f\"\\nTopic Representations After COVID for {ideology}:\")\n",
    "    print(f\"\\nNumber of Topic: {len(topics_info_after)}\")\n",
    "    display(topics_info_after)\n",
    "    # display(model_after.visualize_hierarchy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
